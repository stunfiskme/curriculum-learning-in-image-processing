{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_kg_hide-input":false,"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","execution":{"iopub.execute_input":"2024-06-04T23:00:17.052749Z","iopub.status.busy":"2024-06-04T23:00:17.051605Z","iopub.status.idle":"2024-06-04T23:00:24.628227Z","shell.execute_reply":"2024-06-04T23:00:24.627085Z","shell.execute_reply.started":"2024-06-04T23:00:17.052676Z"},"trusted":true},"outputs":[],"source":["import numpy as np # linear algebra\n","import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n","import matplotlib.pyplot as plt\n","from plotly.subplots import make_subplots\n","import plotly.graph_objs as go\n","import copy\n","import os\n","import torch\n","from PIL import Image\n","from PIL import Image, ImageDraw\n","from torch.utils.data import Dataset, Sampler\n","import torchvision.transforms as transforms\n","from torch.utils.data import random_split\n","from torch.optim.lr_scheduler import ReduceLROnPlateau\n","import torch.nn as nn\n","from torchvision import utils\n","import seaborn as sns; sns.set(style='whitegrid')\n","%matplotlib inline"]},{"cell_type":"code","execution_count":2,"metadata":{"execution":{"iopub.execute_input":"2024-06-04T23:00:24.631437Z","iopub.status.busy":"2024-06-04T23:00:24.630771Z","iopub.status.idle":"2024-06-04T23:00:38.758141Z","shell.execute_reply":"2024-06-04T23:00:38.756804Z","shell.execute_reply.started":"2024-06-04T23:00:24.631394Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Collecting torchsummary\n","  Downloading torchsummary-1.5.1-py3-none-any.whl.metadata (296 bytes)\n","Downloading torchsummary-1.5.1-py3-none-any.whl (2.8 kB)\n","Installing collected packages: torchsummary\n","Successfully installed torchsummary-1.5.1\n"]}],"source":["# library which allows us to view model summary like keras/tf\n","!pip install torchsummary"]},{"cell_type":"code","execution_count":3,"metadata":{"execution":{"iopub.execute_input":"2024-06-04T23:00:38.764438Z","iopub.status.busy":"2024-06-04T23:00:38.764029Z","iopub.status.idle":"2024-06-04T23:00:39.318259Z","shell.execute_reply":"2024-06-04T23:00:39.317165Z","shell.execute_reply.started":"2024-06-04T23:00:38.764393Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["|    | id                                       |   label |\n","|---:|:-----------------------------------------|--------:|\n","|  0 | f38a6374c348f90b587e046aac6079959adf3835 |       0 |\n","|  1 | c18f2d887b7ae4f6742ee445113fa1aef383ed77 |       1 |\n","|  2 | 755db6279dae599ebb4d39a9123cce439965282d |       0 |\n","|  3 | bc3f0c64fb968ff4a8bd33af6971ecae77c75e08 |       0 |\n","|  4 | 068aba587a4950175d04c680d38943fd488d6a9d |       0 |\n"]},{"data":{"text/plain":["label\n","0    130908\n","1     89117\n","Name: count, dtype: int64"]},"execution_count":3,"metadata":{},"output_type":"execute_result"}],"source":["labels_df = pd.read_csv('/kaggle/input/histopathologic-cancer-detection/train_labels.csv')\n","print(labels_df.head().to_markdown())\n","os.listdir('/kaggle/input/histopathologic-cancer-detection/')\n","labels_df.shape\n","# No duplicate ids found\n","labels_df[labels_df.duplicated(keep=False)]\n","labels_df['label'].value_counts()"]},{"cell_type":"code","execution_count":4,"metadata":{"execution":{"iopub.execute_input":"2024-06-04T23:00:39.322609Z","iopub.status.busy":"2024-06-04T23:00:39.321915Z","iopub.status.idle":"2024-06-04T23:00:39.336895Z","shell.execute_reply":"2024-06-04T23:00:39.335625Z","shell.execute_reply.started":"2024-06-04T23:00:39.322569Z"},"trusted":true},"outputs":[],"source":["torch.manual_seed(42) # fix random seed\n","\n","class pytorch_data(Dataset):\n","    \n","    def __init__(self,data_dir,transform,data_type=\"train\"):      \n","    \n","        # Get Image File Names\n","        cdm_data=os.path.join(data_dir,data_type)  # directory of files\n","        \n","        file_names = os.listdir(cdm_data) # get list of images in that directory  \n","        idx_choose = np.random.choice(np.arange(len(file_names)), \n","                                      4000,\n","                                      replace=False).tolist()\n","        file_names_sample = [file_names[x] for x in idx_choose]\n","        self.full_filenames = [os.path.join(cdm_data, f) for f in file_names_sample]   # get the full path to images\n","        \n","        # Get Labels\n","        labels_data=os.path.join(data_dir,\"train_labels.csv\") \n","        labels_df=pd.read_csv(labels_data)\n","        labels_df.set_index(\"id\", inplace=True) # set data frame index to id\n","        self.labels = [labels_df.loc[filename[:-4]].values[0] for filename in file_names_sample]  # obtained labels from df\n","        self.transform = transform\n","      \n","    def __len__(self):\n","        return len(self.full_filenames) # size of dataset\n","      \n","    def __getitem__(self, idx):\n","        # open image, apply transforms and return with label\n","        image = Image.open(self.full_filenames[idx])  # Open Image with PIL\n","        image = self.transform(image) # Apply Specific Transformation to Image\n","        return image, self.labels[idx]\n"]},{"cell_type":"code","execution_count":5,"metadata":{"execution":{"iopub.execute_input":"2024-06-04T23:00:39.338967Z","iopub.status.busy":"2024-06-04T23:00:39.338492Z","iopub.status.idle":"2024-06-04T23:00:39.350213Z","shell.execute_reply":"2024-06-04T23:00:39.349031Z","shell.execute_reply.started":"2024-06-04T23:00:39.338926Z"},"trusted":true},"outputs":[],"source":["# define transformation that converts a PIL image into PyTorch tensors\n","import torchvision.transforms as transforms\n","data_transformer = transforms.Compose([transforms.ToTensor(),\n","                                       transforms.Resize((46,46))])"]},{"cell_type":"code","execution_count":6,"metadata":{"execution":{"iopub.execute_input":"2024-06-04T23:00:39.352138Z","iopub.status.busy":"2024-06-04T23:00:39.351717Z","iopub.status.idle":"2024-06-04T23:00:48.381270Z","shell.execute_reply":"2024-06-04T23:00:48.379848Z","shell.execute_reply.started":"2024-06-04T23:00:39.352103Z"},"trusted":true},"outputs":[],"source":["# Define an object of the custom dataset for the train folder.\n","data_dir = '/kaggle/input/histopathologic-cancer-detection/'\n","img_dataset = pytorch_data(data_dir, data_transformer, \"train\") # Histopathalogic images"]},{"cell_type":"code","execution_count":7,"metadata":{"execution":{"iopub.execute_input":"2024-06-04T23:00:48.384231Z","iopub.status.busy":"2024-06-04T23:00:48.383702Z","iopub.status.idle":"2024-06-04T23:00:48.623961Z","shell.execute_reply":"2024-06-04T23:00:48.622725Z","shell.execute_reply.started":"2024-06-04T23:00:48.384187Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["torch.Size([3, 46, 46]) tensor(0.2142) tensor(0.9954)\n"]},{"name":"stderr","output_type":"stream","text":["/opt/conda/lib/python3.10/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n","  warnings.warn(\n"]}],"source":["# load an example tensor\n","img,label=img_dataset[10]\n","print(img.shape,torch.min(img),torch.max(img))"]},{"cell_type":"code","execution_count":8,"metadata":{"execution":{"iopub.execute_input":"2024-06-04T23:00:48.625926Z","iopub.status.busy":"2024-06-04T23:00:48.625454Z","iopub.status.idle":"2024-06-04T23:00:48.638320Z","shell.execute_reply":"2024-06-04T23:00:48.637032Z","shell.execute_reply.started":"2024-06-04T23:00:48.625883Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["train dataset size: 3200\n","validation dataset size: 800\n"]}],"source":["len_img=len(img_dataset)\n","len_train=int(0.8*len_img)\n","len_val=len_img-len_train\n","\n","# Split Pytorch tensor\n","train_ts,val_ts=random_split(img_dataset,\n","                             [len_train,len_val]) # random split 80/20\n","\n","print(\"train dataset size:\", len(train_ts))\n","print(\"validation dataset size:\", len(val_ts))"]},{"cell_type":"code","execution_count":9,"metadata":{"_kg_hide-input":true,"execution":{"iopub.execute_input":"2024-06-04T23:00:48.641045Z","iopub.status.busy":"2024-06-04T23:00:48.640475Z","iopub.status.idle":"2024-06-04T23:00:49.144295Z","shell.execute_reply":"2024-06-04T23:00:49.143286Z","shell.execute_reply.started":"2024-06-04T23:00:48.641005Z"},"trusted":true},"outputs":[],"source":["import plotly.express as px\n","\n","def plot_img(x,y,title=None):\n","\n","    npimg = x.numpy() # convert tensor to numpy array\n","    npimg_tr=np.transpose(npimg, (1,2,0)) # Convert to H*W*C shape\n","    fig = px.imshow(npimg_tr)\n","    fig.update_layout(template='plotly_white')\n","    fig.update_layout(title=title,height=300,margin={'l':10,'r':20,'b':10})\n","    fig.show()"]},{"cell_type":"code","execution_count":10,"metadata":{"execution":{"iopub.execute_input":"2024-06-04T23:00:49.146129Z","iopub.status.busy":"2024-06-04T23:00:49.145779Z","iopub.status.idle":"2024-06-04T23:00:49.152433Z","shell.execute_reply":"2024-06-04T23:00:49.150778Z","shell.execute_reply.started":"2024-06-04T23:00:49.146100Z"},"trusted":true},"outputs":[],"source":["# Define the following transformations for the training dataset\n","tr_transf = transforms.Compose([\n","#     transforms.Resize((40,40)),\n","    transforms.RandomHorizontalFlip(p=0.5), \n","    transforms.RandomVerticalFlip(p=0.5),  \n","    transforms.RandomRotation(45),         \n","#     transforms.RandomResizedCrop(50,scale=(0.8,1.0),ratio=(1.0,1.0)),\n","    transforms.ToTensor()])"]},{"cell_type":"code","execution_count":11,"metadata":{"execution":{"iopub.execute_input":"2024-06-04T23:00:49.154474Z","iopub.status.busy":"2024-06-04T23:00:49.153973Z","iopub.status.idle":"2024-06-04T23:00:49.165813Z","shell.execute_reply":"2024-06-04T23:00:49.164764Z","shell.execute_reply.started":"2024-06-04T23:00:49.154434Z"},"trusted":true},"outputs":[],"source":["# For the validation dataset, we don't need any augmentation; simply convert images into tensors\n","val_transf = transforms.Compose([\n","    transforms.ToTensor()])\n","\n","# After defining the transformations, overwrite the transform functions of train_ts, val_ts\n","train_ts.transform=tr_transf\n","val_ts.transform=val_transf"]},{"cell_type":"code","execution_count":12,"metadata":{"execution":{"iopub.execute_input":"2024-06-04T23:00:49.167354Z","iopub.status.busy":"2024-06-04T23:00:49.166988Z","iopub.status.idle":"2024-06-04T23:00:49.181829Z","shell.execute_reply":"2024-06-04T23:00:49.180703Z","shell.execute_reply.started":"2024-06-04T23:00:49.167327Z"},"trusted":true},"outputs":[{"data":{"text/plain":["Compose(\n","    RandomHorizontalFlip(p=0.5)\n","    RandomVerticalFlip(p=0.5)\n","    RandomRotation(degrees=[-45.0, 45.0], interpolation=nearest, expand=False, fill=0)\n","    ToTensor()\n",")"]},"execution_count":12,"metadata":{},"output_type":"execute_result"}],"source":["# The subset can also have transform attribute (if we asign)\n","train_ts.transform"]},{"cell_type":"code","execution_count":13,"metadata":{"execution":{"iopub.execute_input":"2024-06-04T23:00:49.186681Z","iopub.status.busy":"2024-06-04T23:00:49.186220Z","iopub.status.idle":"2024-06-04T23:00:49.292583Z","shell.execute_reply":"2024-06-04T23:00:49.291424Z","shell.execute_reply.started":"2024-06-04T23:00:49.186649Z"},"trusted":true},"outputs":[],"source":["import pywt\n","#Sorter\n","class RMSESampler(Sampler[int]):\n","    \n","    def __init__(self, data: torch.utils.data.Subset) -> None:\n","        self.data = data\n","        \n","    def __len__(self) -> int:\n","        return len(self.data)\n","    \n","    def __iter__(self): # -> iter[int]:\n","        images = []\n","        for i in self.data.indices:\n","            images.append(self.data.dataset.__getitem__(i))\n","        rmse = []\n","        for img in images:\n","            img_array = img[0].numpy().copy()\n","            img_shape = img_array.shape\n","            img_resize = np.resize(np.resize(img_array, (img_shape[0] // 2, img_shape[1] // 2, img_shape[2] // 2)),\n","                (img_shape[0], img_shape[1], img_shape[2])\n","            )\n","            img_resize_array = np.asarray(img_resize)\n","\n","            # RMSE\n","            # Get the RMSE between the original and the resized version\n","            img_rmse = np.sqrt(np.mean((img_array - img_resize_array) ** 2))\n","            rmse.append(img_rmse)\n","        rmse = np.negative(np.abs(rmse - np.mean(rmse)))\n","        rmse = torch.from_numpy(np.asarray(rmse))\n","        yield from torch.argsort(rmse).tolist()\n","        \n","class WDSampler(Sampler[int]):\n","    \n","    def __init__(self, data: torch.utils.data.Subset) -> None:\n","        self.data = data\n","        \n","    def __len__(self) -> int:\n","        return len(self.data)\n","    \n","    def __iter__(self): # -> iter[int]:\n","        images = []\n","        for i in self.data.indices:\n","            images.append(self.data.dataset.__getitem__(i))\n","        decomp = []\n","        for img in images:\n","            img_array = img[0].numpy().copy()\n","            wd = pywt.wavedec2(img_array, \"db2\", level=1)\n","            decomp.append(np.sum((np.square(wd[1][0]), np.square(wd[1][1]), np.square(wd[1][2]))))\n","#         decomp = np.abs(decomp - np.mean(decomp))\n","#         decomp = np.negative(np.abs(decomp - np.mean(decomp)))\n","#         decomp = torch.from_numpy(np.array(decomp))\n","        decomp = torch.from_numpy(np.negative(np.array(decomp)))\n","        yield from torch.argsort(decomp).tolist()\n","        \n","class RMSEBatchSampler(Sampler[list[int]]):\n","    \n","    def __init__(self, data: list[str], batch_size: int) -> None:\n","        self.data = data\n","        self.batch_size = batch_size\n","        \n","    def __len__(self) -> int:\n","        return (len(self.data) + self.batch_size - 1) // self.batch_size\n","    \n","    def __iter__(self):\n","        images = []\n","        for i in self.data.indices:\n","            images.append(self.data.dataset.__getitem__(i))\n","        rmse = []\n","        for img in images:\n","            img_array = img[0].numpy().copy()\n","            img_shape = img_array.shape\n","            img_resize = np.resize(np.resize(img_array, (img_shape[0] // 2, img_shape[1] // 2, img_shape[2] // 2)),\n","                (img_shape[0], img_shape[1], img_shape[2])\n","            )\n","            img_resize_array = np.asarray(img_resize)\n","\n","            # RMSE\n","            # Get the RMSE between the original and the resized version\n","            img_rmse = np.sqrt(np.mean((img_array - img_resize_array) ** 2))\n","            rmse.append(img_rmse)\n","        rmse = np.negative(np.abs(rmse - np.mean(rmse)))\n","        rmse = torch.from_numpy(rmse)\n","        for batch in torch.chunk(torch.argsort(rmse), len(self)):\n","            yield batch.tolist()\n","            \n","class WDBatchSampler(Sampler[list[int]]):\n","    \n","    def __init__(self, data: list[str], batch_size: int) -> None:\n","        self.data = data\n","        self.batch_size = batch_size\n","        \n","    def __len__(self) -> int:\n","        return (len(self.data) + self.batch_size - 1) // self.batch_size\n","    \n","    def __iter__(self):\n","        images = []\n","        for i in self.data.indices:\n","            images.append(self.data.dataset.__getitem__(i))\n","        decomp = []\n","        for img in images:\n","            img_array = img[0].numpy().copy()\n","            wd = pywt.wavedec2(img_array, \"db2\", level=3)\n","            decomp.append(np.sum((np.square(wd[3][0]), np.square(wd[3][1]), np.square(wd[3][2])))) #np.square(wd[1][0]), np.square(wd[1][1]), np.square(wd[1][2])\n","#         decomp = np.abs(decomp - np.mean(decomp))\n","        decomp = np.negative(np.abs(decomp - np.mean(decomp)))\n","#         decomp = torch.from_numpy(np.array(decomp))\n","#         decomp = torch.from_numpy(np.negative(np.array(decomp)))\n","        for batch in torch.chunk(torch.argsort(torch.from_numpy(decomp)), len(self)):\n","            yield batch.tolist()"]},{"cell_type":"code","execution_count":14,"metadata":{"execution":{"iopub.execute_input":"2024-06-04T23:00:49.295038Z","iopub.status.busy":"2024-06-04T23:00:49.294083Z","iopub.status.idle":"2024-06-04T23:00:49.302220Z","shell.execute_reply":"2024-06-04T23:00:49.300870Z","shell.execute_reply.started":"2024-06-04T23:00:49.294989Z"},"trusted":true},"outputs":[],"source":["from torch.utils.data import DataLoader\n","\n","# # Training DataLoader\n","# train_dl = DataLoader(train_ts, \n","#                       shuffle=False,\n","#                       batch_sampler=WTFBatchSampler(train_ts, 16))\n","\n","# Training DataLoader\n","train_dl = DataLoader(train_ts,\n","                      batch_size=16, \n","                      shuffle=False)\n","\n","# Validation DataLoader\n","val_dl = DataLoader(val_ts,\n","                    batch_size=16,\n","                    shuffle=False)"]},{"cell_type":"code","execution_count":15,"metadata":{"execution":{"iopub.execute_input":"2024-06-04T23:00:49.304501Z","iopub.status.busy":"2024-06-04T23:00:49.304016Z","iopub.status.idle":"2024-06-04T23:00:49.475824Z","shell.execute_reply":"2024-06-04T23:00:49.474765Z","shell.execute_reply.started":"2024-06-04T23:00:49.304456Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["torch.Size([16, 3, 46, 46]) tensor([0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0])\n"]}],"source":["# check samples\n","for x,y in train_dl:\n","    print(x.shape,y)\n","    break"]},{"cell_type":"code","execution_count":16,"metadata":{"_kg_hide-input":false,"execution":{"iopub.execute_input":"2024-06-04T23:00:49.477786Z","iopub.status.busy":"2024-06-04T23:00:49.477389Z","iopub.status.idle":"2024-06-04T23:00:49.491961Z","shell.execute_reply":"2024-06-04T23:00:49.490746Z","shell.execute_reply.started":"2024-06-04T23:00:49.477755Z"},"trusted":true},"outputs":[],"source":["def findConv2dOutShape(hin,win,conv,pool=2):\n","    # get conv arguments\n","    kernel_size=conv.kernel_size\n","    stride=conv.stride\n","    padding=conv.padding\n","    dilation=conv.dilation\n","\n","    hout=np.floor((hin+2*padding[0]-dilation[0]*(kernel_size[0]-1)-1)/stride[0]+1)\n","    wout=np.floor((win+2*padding[1]-dilation[1]*(kernel_size[1]-1)-1)/stride[1]+1)\n","\n","    if pool:\n","        hout/=pool\n","        wout/=pool\n","    return int(hout),int(wout)\n","\n","import torch.nn as nn\n","import torch.nn.functional as F\n","\n","# Neural Network\n","class Network(nn.Module):\n","    \n","    # Network Initialisation\n","    def __init__(self, params):\n","        \n","        super(Network, self).__init__()\n","    \n","        Cin,Hin,Win=params[\"shape_in\"]\n","        init_f=params[\"initial_filters\"] \n","        num_fc1=params[\"num_fc1\"]  \n","        num_classes=params[\"num_classes\"] \n","        self.dropout_rate=params[\"dropout_rate\"] \n","        \n","        # Convolution Layers\n","        self.conv1 = nn.Conv2d(Cin, init_f, kernel_size=3)\n","        h,w=findConv2dOutShape(Hin,Win,self.conv1)\n","        self.conv2 = nn.Conv2d(init_f, 2*init_f, kernel_size=3)\n","        h,w=findConv2dOutShape(h,w,self.conv2)\n","        self.conv3 = nn.Conv2d(2*init_f, 4*init_f, kernel_size=3)\n","        h,w=findConv2dOutShape(h,w,self.conv3)\n","        self.conv4 = nn.Conv2d(4*init_f, 8*init_f, kernel_size=3)\n","        h,w=findConv2dOutShape(h,w,self.conv4)\n","        \n","        # compute the flatten size\n","        self.num_flatten=h*w*8*init_f\n","        self.fc1 = nn.Linear(self.num_flatten, num_fc1)\n","        self.fc2 = nn.Linear(num_fc1, num_classes)\n","\n","    def forward(self,X):\n","        \n","        # Convolution & Pool Layers\n","        X = F.relu(self.conv1(X)); \n","        X = F.max_pool2d(X, 2, 2)\n","        X = F.relu(self.conv2(X))\n","        X = F.max_pool2d(X, 2, 2)\n","        X = F.relu(self.conv3(X))\n","        X = F.max_pool2d(X, 2, 2)\n","        X = F.relu(self.conv4(X))\n","        X = F.max_pool2d(X, 2, 2)\n","\n","        X = X.view(-1, self.num_flatten)\n","        \n","        X = F.relu(self.fc1(X))\n","        X=F.dropout(X, self.dropout_rate)\n","        X = self.fc2(X)\n","        return F.log_softmax(X, dim=1)"]},{"cell_type":"code","execution_count":17,"metadata":{"execution":{"iopub.execute_input":"2024-06-04T23:00:49.493566Z","iopub.status.busy":"2024-06-04T23:00:49.493234Z","iopub.status.idle":"2024-06-04T23:00:49.522619Z","shell.execute_reply":"2024-06-04T23:00:49.521399Z","shell.execute_reply.started":"2024-06-04T23:00:49.493539Z"},"trusted":true},"outputs":[],"source":["# Neural Network Predefined Parameters\n","params_model={\n","        \"shape_in\": (3,46,46), \n","        \"initial_filters\": 8,    \n","        \"num_fc1\": 100,\n","        \"dropout_rate\": 0.25,\n","        \"num_classes\": 2}\n","\n","# Create instantiation of Network class\n","cnn_model = Network(params_model)\n","\n","# define computation hardware approach (GPU/CPU)\n","device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","model = cnn_model.to(device)"]},{"cell_type":"code","execution_count":18,"metadata":{"execution":{"iopub.execute_input":"2024-06-04T23:00:49.524410Z","iopub.status.busy":"2024-06-04T23:00:49.524041Z","iopub.status.idle":"2024-06-04T23:00:49.636092Z","shell.execute_reply":"2024-06-04T23:00:49.635004Z","shell.execute_reply.started":"2024-06-04T23:00:49.524380Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["----------------------------------------------------------------\n","        Layer (type)               Output Shape         Param #\n","================================================================\n","            Conv2d-1            [-1, 8, 44, 44]             224\n","            Conv2d-2           [-1, 16, 20, 20]           1,168\n","            Conv2d-3             [-1, 32, 8, 8]           4,640\n","            Conv2d-4             [-1, 64, 2, 2]          18,496\n","            Linear-5                  [-1, 100]           6,500\n","            Linear-6                    [-1, 2]             202\n","================================================================\n","Total params: 31,230\n","Trainable params: 31,230\n","Non-trainable params: 0\n","----------------------------------------------------------------\n","Input size (MB): 0.02\n","Forward/backward pass size (MB): 0.19\n","Params size (MB): 0.12\n","Estimated Total Size (MB): 0.33\n","----------------------------------------------------------------\n"]}],"source":["from torchsummary import summary\n","summary(cnn_model, input_size=(3, 46, 46),device=device.type)"]},{"cell_type":"code","execution_count":19,"metadata":{"execution":{"iopub.execute_input":"2024-06-04T23:00:49.637770Z","iopub.status.busy":"2024-06-04T23:00:49.637394Z","iopub.status.idle":"2024-06-04T23:00:49.643019Z","shell.execute_reply":"2024-06-04T23:00:49.641752Z","shell.execute_reply.started":"2024-06-04T23:00:49.637739Z"},"trusted":true},"outputs":[],"source":["loss_func = nn.NLLLoss(reduction=\"sum\")"]},{"cell_type":"code","execution_count":20,"metadata":{"execution":{"iopub.execute_input":"2024-06-04T23:00:49.644831Z","iopub.status.busy":"2024-06-04T23:00:49.644380Z","iopub.status.idle":"2024-06-04T23:00:49.654864Z","shell.execute_reply":"2024-06-04T23:00:49.653777Z","shell.execute_reply.started":"2024-06-04T23:00:49.644792Z"},"trusted":true},"outputs":[],"source":["from torch import optim\n","opt = optim.Adam(cnn_model.parameters(), lr=3e-4)\n","lr_scheduler = ReduceLROnPlateau(opt, mode='min',factor=0.5, patience=20,verbose=1)"]},{"cell_type":"code","execution_count":21,"metadata":{"_kg_hide-input":true,"execution":{"iopub.execute_input":"2024-06-04T23:00:49.656493Z","iopub.status.busy":"2024-06-04T23:00:49.656115Z","iopub.status.idle":"2024-06-04T23:00:49.668978Z","shell.execute_reply":"2024-06-04T23:00:49.667647Z","shell.execute_reply.started":"2024-06-04T23:00:49.656460Z"},"trusted":true},"outputs":[],"source":["''' Helper Functions'''\n","\n","        \n","# Function to get the learning rate\n","def get_lr(opt):\n","    for param_group in opt.param_groups:\n","        return param_group['lr']\n","\n","# Function to compute the loss value per batch of data\n","def loss_batch(loss_func, output, target, opt=None):\n","    \n","    loss = loss_func(output, target) # get loss\n","    pred = output.argmax(dim=1, keepdim=True) # Get Output Class\n","    metric_b=pred.eq(target.view_as(pred)).sum().item() # get performance metric\n","    \n","    if opt is not None:\n","        opt.zero_grad()\n","        loss.backward()\n","        opt.step()\n","\n","    return loss.item(), metric_b\n","\n","# Compute the loss value & performance metric for the entire dataset (epoch)\n","def loss_epoch(model,loss_func,dataset_dl,opt=None):\n","    \n","    run_loss=0.0 \n","    t_metric=0.0\n","    len_data=len(dataset_dl.dataset)\n","\n","    loss_stats = []\n","    \n","    # internal loop over dataset\n","    for xb, yb in dataset_dl:\n","        # move batch to device\n","        xb=xb.to(device)\n","        yb=yb.to(device)\n","        output=model(xb) # get model output\n","        loss_b,metric_b=loss_batch(loss_func, output, yb, opt) # get loss per batch\n","        run_loss+=loss_b        # update running loss\n","            \n","        if metric_b is not None: # update running metric\n","            t_metric+=metric_b    \n","    \n","    loss=run_loss/float(len_data)  # average loss value\n","    metric=t_metric/float(len_data) # average metric value\n","    \n","    return loss, metric"]},{"cell_type":"code","execution_count":22,"metadata":{"execution":{"iopub.execute_input":"2024-06-04T23:00:49.671394Z","iopub.status.busy":"2024-06-04T23:00:49.670961Z","iopub.status.idle":"2024-06-04T23:00:49.688569Z","shell.execute_reply":"2024-06-04T23:00:49.687420Z","shell.execute_reply.started":"2024-06-04T23:00:49.671355Z"},"trusted":true},"outputs":[],"source":["from tqdm.notebook import trange, tqdm\n","\n","def train_val(model, params,verbose=False):\n","    \n","    # Get the parameters\n","    epochs=params[\"epochs\"]\n","    loss_func=params[\"f_loss\"]\n","    opt=params[\"optimiser\"]\n","    train_dl=params[\"train\"]\n","    val_dl=params[\"val\"]\n","    lr_scheduler=params[\"lr_change\"]\n","    weight_path=params[\"weight_path\"]\n","    \n","    loss_history={\"train\": [],\"val\": []} # history of loss values in each epoch\n","    metric_history={\"train\": [],\"val\": []} # histroy of metric values in each epoch\n","    best_model_wts = copy.deepcopy(model.state_dict()) # a deep copy of weights for the best performing model\n","    best_loss=float('inf') # initialize best loss to a large value\n","    \n","    ''' Train Model n_epochs '''\n","    \n","    for epoch in tqdm(range(epochs)):\n","        \n","        ''' Get the Learning Rate '''\n","        current_lr=get_lr(opt)\n","        if(verbose):\n","            print('Epoch {}/{}, current lr={}'.format(epoch, epochs - 1, current_lr))\n","        \n","        '''\n","        \n","        Train Model Process\n","        \n","        '''\n","        \n","        model.train()\n","        train_loss, train_metric = loss_epoch(model,loss_func,train_dl,opt)\n","\n","        # collect losses\n","        loss_history[\"train\"].append(train_loss)\n","        metric_history[\"train\"].append(train_metric)\n","        \n","        '''\n","        \n","        Evaluate Model Process\n","        \n","        '''\n","        \n","        model.eval()\n","        with torch.no_grad():\n","            val_loss, val_metric = loss_epoch(model,loss_func,val_dl)\n","        \n","        # store best model\n","        if(val_loss < best_loss):\n","            best_loss = val_loss\n","            best_model_wts = copy.deepcopy(model.state_dict())\n","            \n","            # store weights into a local file\n","            torch.save(model.state_dict(), weight_path)\n","            if(verbose):\n","                print(\"Copied best model weights!\")\n","        \n","        # collect loss and metric for validation dataset\n","        loss_history[\"val\"].append(val_loss)\n","        metric_history[\"val\"].append(val_metric)\n","        \n","        # learning rate schedule\n","        lr_scheduler.step(val_loss)\n","        if current_lr != get_lr(opt):\n","            if(verbose):\n","                print(\"Loading best model weights!\")\n","                model.load_state_dict(best_model_wts) \n","\n","        if(verbose):\n","            print(f\"train loss: {train_loss:.6f}, dev loss: {val_loss:.6f}, accuracy: {100*val_metric:.2f}\")\n","            print(\"-\"*10) \n","\n","    # load best model weights\n","    model.load_state_dict(best_model_wts)\n","        \n","    return model, loss_history, metric_history"]},{"cell_type":"code","execution_count":23,"metadata":{"execution":{"iopub.execute_input":"2024-06-04T23:00:49.690618Z","iopub.status.busy":"2024-06-04T23:00:49.690157Z","iopub.status.idle":"2024-06-04T23:00:49.703269Z","shell.execute_reply":"2024-06-04T23:00:49.702239Z","shell.execute_reply.started":"2024-06-04T23:00:49.690578Z"},"trusted":true},"outputs":[],"source":["# params_train={\n","#  \"train\": train_dl,\"val\": val_dl,\n","#  \"epochs\": 50,\n","#  \"optimiser\": optim.Adam(cnn_model.parameters(),lr=3e-4),\n","#  \"lr_change\": ReduceLROnPlateau(opt,\n","#                                 mode='min',\n","#                                 factor=0.5,\n","#                                 patience=20,\n","#                                 verbose=1),\n","#  \"f_loss\": nn.NLLLoss(reduction=\"sum\"),\n","#  \"weight_path\": \"weights.pt\",\n","# }\n","\n","# ''' Actual Train / Evaluation of CNN Model '''\n","# # train and validate the model\n","\n","# cnn_model,loss_hist,metric_hist=train_val(cnn_model,params_train, True)"]},{"cell_type":"code","execution_count":24,"metadata":{"_kg_hide-input":true,"execution":{"iopub.execute_input":"2024-06-04T23:00:49.706665Z","iopub.status.busy":"2024-06-04T23:00:49.704846Z","iopub.status.idle":"2024-06-04T23:00:49.720180Z","shell.execute_reply":"2024-06-04T23:00:49.719058Z","shell.execute_reply.started":"2024-06-04T23:00:49.706623Z"},"trusted":true},"outputs":[],"source":["# import seaborn as sns; sns.set(style='whitegrid')\n","\n","# epochs=params_train[\"epochs\"]\n","\n","# fig,ax = plt.subplots(1,2,figsize=(12,5))\n","\n","# sns.lineplot(x=[*range(1,epochs+1)],y=loss_hist[\"train\"],ax=ax[0],label='loss_hist[\"train\"]')\n","# sns.lineplot(x=[*range(1,epochs+1)],y=loss_hist[\"val\"],ax=ax[0],label='loss_hist[\"val\"]')\n","# sns.lineplot(x=[*range(1,epochs+1)],y=metric_hist[\"train\"],ax=ax[1],label='metric_hist[\"train\"]')\n","# sns.lineplot(x=[*range(1,epochs+1)],y=metric_hist[\"val\"],ax=ax[1],label='metric_hist[\"val\"]')\n","# plt.title('Convergence History')"]},{"cell_type":"code","execution_count":25,"metadata":{"execution":{"iopub.execute_input":"2024-06-04T23:00:49.722268Z","iopub.status.busy":"2024-06-04T23:00:49.721746Z","iopub.status.idle":"2024-06-04T23:00:49.734124Z","shell.execute_reply":"2024-06-04T23:00:49.732932Z","shell.execute_reply.started":"2024-06-04T23:00:49.722228Z"},"trusted":true},"outputs":[],"source":["import warnings\n","warnings.filterwarnings(\"ignore\", \"is_categorical_dtype\")\n","warnings.filterwarnings(\"ignore\", \"use_inf_as_na\")"]},{"cell_type":"code","execution_count":26,"metadata":{"execution":{"iopub.execute_input":"2024-06-04T23:00:49.736241Z","iopub.status.busy":"2024-06-04T23:00:49.735853Z","iopub.status.idle":"2024-06-04T23:01:21.832726Z","shell.execute_reply":"2024-06-04T23:01:21.830781Z","shell.execute_reply.started":"2024-06-04T23:00:49.736209Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["/opt/conda/lib/python3.10/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["TRAINING DATASET STATS FOR LEVEL 1:\n","\n","    HORIZONTAL: Mean: 3.049999952316284\tSt.d.: 0.8799999952316284\n","\n","    VERTICAL:   Mean: 0.03999999910593033\tSt.d.: 0.36000001430511475\n","\n","    DIAGONAL:   Mean: 0.009999999776482582\tSt.d.: 0.25\n","\n","TRAINING DATASET STATS FOR LEVEL 2:\n","\n","    HORIZONTAL: Mean: 4.070000171661377\tSt.d.: 1.2699999809265137\n","\n","    VERTICAL:   Mean: 0.009999999776482582\tSt.d.: 0.20999999344348907\n","\n","    DIAGONAL:   Mean: 0.0\tSt.d.: 0.14000000059604645\n","\n","TRAINING DATASET STATS FOR LEVEL 3:\n","\n","    HORIZONTAL: Mean: 4.28000020980835\tSt.d.: 0.8600000143051147\n","\n","    VERTICAL:   Mean: -0.0\tSt.d.: 0.25999999046325684\n","\n","    DIAGONAL:   Mean: 0.009999999776482582\tSt.d.: 0.20999999344348907\n","\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"bf35fd0e442d4e328c3ac65d9501eb63","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/3 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["|||||||||||||||||||||||||||||||||||||||||||||||||||||||||\n","LEVEL 1\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"ce7d3fec1a3541b79d56441b04840ffb","version_major":2,"version_minor":0},"text/plain":["0it [00:00, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["---------------------------------------------------------\n","SORTING from Middle\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"b63343bb71e04f6da998ebcadd6cbf71","version_major":2,"version_minor":0},"text/plain":["0it [00:00, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["[[3.05, 0.04, 0.01], [4.07, 0.01, 0.0], [4.28, -0.0, 0.01]]\n",".........................................................\n","SUMMATION Horizontal\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"afcb9a2706df436a8e80320e5c6d39c5","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/10 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"ename":"TypeError","evalue":"loss_epoch() takes from 3 to 4 positional arguments but 5 were given","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)","Cell \u001b[0;32mIn[26], line 185\u001b[0m\n\u001b[1;32m    182\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m''' Actual Train / Evaluation of CNN Model '''\u001b[39;00m\n\u001b[1;32m    183\u001b[0m \u001b[38;5;66;03m# train and validate the model\u001b[39;00m\n\u001b[0;32m--> 185\u001b[0m cnn_model,loss_hist,metric_hist\u001b[38;5;241m=\u001b[39m\u001b[43mtrain_val\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcnn_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43mparams_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    187\u001b[0m epochs\u001b[38;5;241m=\u001b[39mparams_train[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mepochs\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m    189\u001b[0m fig,ax \u001b[38;5;241m=\u001b[39m plt\u001b[38;5;241m.\u001b[39msubplots(\u001b[38;5;241m1\u001b[39m,\u001b[38;5;241m2\u001b[39m,figsize\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m12\u001b[39m,\u001b[38;5;241m5\u001b[39m))\n","Cell \u001b[0;32mIn[22], line 35\u001b[0m, in \u001b[0;36mtrain_val\u001b[0;34m(model, params, verbose)\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m'''\u001b[39;00m\n\u001b[1;32m     29\u001b[0m \u001b[38;5;124;03m\u001b[39;00m\n\u001b[1;32m     30\u001b[0m \u001b[38;5;124;03mTrain Model Process\u001b[39;00m\n\u001b[1;32m     31\u001b[0m \u001b[38;5;124;03m\u001b[39;00m\n\u001b[1;32m     32\u001b[0m \u001b[38;5;124;03m'''\u001b[39;00m\n\u001b[1;32m     34\u001b[0m model\u001b[38;5;241m.\u001b[39mtrain()\n\u001b[0;32m---> 35\u001b[0m train_loss, train_metric, mean_loss_complexity \u001b[38;5;241m=\u001b[39m \u001b[43mloss_epoch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43mloss_func\u001b[49m\u001b[43m,\u001b[49m\u001b[43mtrain_dl\u001b[49m\u001b[43m,\u001b[49m\u001b[43mopt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m     37\u001b[0m \u001b[38;5;66;03m# collect losses\u001b[39;00m\n\u001b[1;32m     38\u001b[0m loss_history[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mappend(train_loss)\n","\u001b[0;31mTypeError\u001b[0m: loss_epoch() takes from 3 to 4 positional arguments but 5 were given"]}],"source":["train_dataset_mean = [[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0]]\n","train_dataset_std = [[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0]]\n","\n","try:\n","    cnn_model.load_state_dict(reset_weights)\n","except:\n","    reset_weights = copy.deepcopy(cnn_model.state_dict())\n","    \n","# sortings\n","def decCalcMiddle(decomp, mean):\n","    return np.abs(np.subtract(decomp, mean))\n","def decCalcMiddleReversed(decomp, mean):\n","    return np.negative(np.abs(np.subtract(decomp, mean)))\n","def decCalcOrder(decomp, mean):\n","    return np.array(decomp)\n","def decCalcOrderReversed(decomp, mean):\n","    return np.negative(np.array(decomp))\n","\n","sortings = [decCalcMiddle, decCalcOrder] # [decCalcMiddle, decCalcMiddleReversed, decCalcOrder, decCalcOrderReversed]\n","sortnames = [\"from Middle\", \"in Order\"] #[\"from Middle\", \"from Middle, reversed\", \"in Order\", \"in Reverse Order\"]\n","\n","# sums\n","\n","def sumOfLevels(wd, level, all_levels = False):\n","    if level == 0:\n","        return np.sum((np.square(wd)))\n","    if all_levels:\n","        sum_of_levels = 0\n","        for l in range(level): #weird subtraction and adding here so that the approx doesn't get messed up\n","            sum_of_levels+=np.sum((np.square(wd[l])))\n","        return sum_of_levels\n","    else:\n","        return np.sum((np.square(wd[level])))\n","\n","def horizontal(wd, level, all_levels = False):\n","    if level == 0:\n","        return np.sum((np.square(wd)))\n","    if all_levels:\n","        sum_of_levels = 0\n","        for l in range(level-1):\n","            if l == 0:\n","                sum_of_levels+=np.square(wd[l])\n","                continue\n","            sum_of_levels+=np.square(wd[l][0])\n","        return sum_of_levels\n","    else:\n","        return np.sum((np.square(wd[level][0])))\n","    \n","def vertical(wd, level, all_levels = False):\n","    if level == 0:\n","        return np.sum((np.square(wd)))\n","    if all_levels:\n","        sum_of_levels = 0\n","        for l in range(level-1):\n","            if l == 0:\n","                sum_of_levels+=np.square(wd[l])\n","                continue\n","            sum_of_levels+=np.square(wd[l][1])\n","        return sum_of_levels\n","    else:\n","        return np.sum((np.square(wd[level][1])))\n","    \n","def diagonal(wd, level, all_levels = False):\n","    if level == 0:\n","        return np.sum((np.square(wd)))\n","    if all_levels:\n","        sum_of_levels = 0\n","        for l in range(level-1):\n","            if l == 0:\n","                sum_of_levels+=np.square(wd[l])\n","                continue\n","            sum_of_levels+=np.square(wd[l][2])\n","        return sum_of_levels\n","    else:\n","        return np.sum((np.square(wd[level][2])))\n","\n","sums = [horizontal, vertical, diagonal]#[sumOfLevels, horizontal, vertical, diagonal]\n","sumnames = [\"Horizontal\", \"Vertical\", \"Diagonal\"] #[\"All directions\", \"Horizontal\", \"Vertical\", \"Diagonal\"]\n","\n","reset_weights = copy.deepcopy(cnn_model.state_dict())\n","\n","img_stats = []\n","\n","\n","\n","for img_array in train_dl:\n","    for img in img_array[0]:\n","        img_stats.append(pywt.wavedec2(img, \"db2\", level=3))\n","\n","for lev in range(1,4):\n","    for direction in range(0,3):\n","        train_dataset_mean[lev-1][direction] = np.round(np.mean(img_stats[:][lev][direction]), 2)\n","        train_dataset_std[lev-1][direction] = np.round(np.std(img_stats[:][lev][direction]),2)\n","    print(f\"\"\"TRAINING DATASET STATS FOR LEVEL {lev}:\\n\n","    HORIZONTAL: Mean: {train_dataset_mean[lev-1][0]}\\tSt.d.: {train_dataset_std[lev-1][0]}\\n\n","    VERTICAL:   Mean: {train_dataset_mean[lev-1][1]}\\tSt.d.: {train_dataset_std[lev-1][1]}\\n\n","    DIAGONAL:   Mean: {train_dataset_mean[lev-1][2]}\\tSt.d.: {train_dataset_std[lev-1][2]}\\n\"\"\")\n","            \n","for lev in tqdm(range(1,4)):\n","    print(\"|||||||||||||||||||||||||||||||||||||||||||||||||||||||||\")\n","    print(f\"LEVEL {lev}\")\n","    for sort, sortname in tqdm(zip(sortings, sortnames)):\n","        print(\"---------------------------------------------------------\")\n","        print(f\"SORTING {sortname}\")\n","        j=0\n","        for summation, sumname in tqdm(zip(sums, sumnames)):\n","            print(train_dataset_mean)\n","            mean = train_dataset_mean[lev][j] # FHIX  \n","            cnn_model.load_state_dict(reset_weights)\n","            print(\".........................................................\")\n","            print(f\"SUMMATION {sumname}\")\n","\n","            class WDSampler(Sampler[int]):\n","\n","                def __init__(self, data: torch.utils.data.Subset) -> None:\n","                    self.data = data\n","\n","                def __len__(self) -> int:\n","                    return len(self.data)\n","\n","                def __iter__(self): # -> iter[int]:\n","                    images = []\n","                    for i in self.data.indices:\n","                        images.append(self.data.dataset.__getitem__(i))\n","                    decomp = []\n","                    for img in images:\n","                        img_array = img[0].numpy().copy()\n","                        wd = pywt.wavedec2(img_array, \"db2\", level=lev)\n","                        decomp.append(summation(wd, lev))\n","                    decomp = sort(decomp)\n","                    yield from torch.argsort(torch.from_numpy(decomp)).tolist()\n","\n","            class WDBatchSampler(Sampler[list[int]]):\n","\n","                def __init__(self, data: list[str], batch_size: int) -> None:\n","                    self.data = data\n","                    self.batch_size = batch_size\n","\n","                def __len__(self) -> int:\n","                    return (len(self.data) + self.batch_size - 1) // self.batch_size\n","\n","                def __iter__(self):\n","                    images = []\n","                    for i in self.data.indices:\n","                        images.append(self.data.dataset.__getitem__(i))\n","\n","                    decomp = []\n","                    for img in images:\n","                        img_array = img[0].numpy().copy()\n","                        wd = pywt.wavedec2(img_array, \"db2\", level=lev)\n","                        decomp.append(summation(wd, lev))\n","                    decomp = sort(decomp, train_dataset_mean[lev][j])\n","                    for batch in torch.chunk(torch.argsort(torch.from_numpy(decomp)), len(self)):\n","                        yield batch.tolist()\n","\n","            train_dl = DataLoader(train_ts, \n","                                  shuffle=False,\n","                                  batch_sampler=WDBatchSampler(train_ts, 16))\n","\n","#                 train_dl = DataLoader(train_ts, \n","#                                       shuffle=False,\n","#                                       batch_size=16,\n","#                                       sampler=WDSampler(train_ts))\n","\n","#                 train_dl = DataLoader(train_ts, \n","#                                       shuffle=False,\n","#                                       batch_size=16)\n","\n","            params_train={\n","             \"train\": train_dl,\"val\": val_dl,\n","             \"epochs\": 10,\n","             \"optimiser\": optim.Adam(cnn_model.parameters(),lr=3e-4),\n","             \"lr_change\": ReduceLROnPlateau(opt,\n","                                            mode='min',\n","                                            factor=0.5,\n","                                            patience=20,\n","                                            verbose=1),\n","             \"f_loss\": nn.NLLLoss(reduction=\"sum\"),\n","             \"weight_path\": \"weights.pt\",\n","            }\n","\n","            ''' Actual Train / Evaluation of CNN Model '''\n","            # train and validate the model\n","\n","            cnn_model,loss_hist,metric_hist=train_val(cnn_model,params_train)\n","\n","            epochs=params_train[\"epochs\"]\n","\n","            fig,ax = plt.subplots(1,2,figsize=(12,5))\n","\n","            sns.lineplot(x=[*range(1,epochs+1)],y=loss_hist[\"train\"],ax=ax[0],label='loss_hist[\"train\"]')\n","            sns.lineplot(x=[*range(1,epochs+1)],y=loss_hist[\"val\"],ax=ax[0],label='loss_hist[\"val\"]')\n","            sns.lineplot(x=[*range(1,epochs+1)],y=metric_hist[\"train\"],ax=ax[1],label='metric_hist[\"train\"]')\n","            sns.lineplot(x=[*range(1,epochs+1)],y=metric_hist[\"val\"],ax=ax[1],label='metric_hist[\"val\"]')\n","            plt.title(f\"LEVEL {lev} {sortname} on {sumname}\")\n","            j+=1"]},{"cell_type":"markdown","metadata":{},"source":["## <b>11 <span style='color:#F1A424'>|</span> Inference</b> \n","\n","- Once we have trained our model using `train_val`, we can begin to utilise it to **<span style='color:#F1A424'>make some predictions</span>**\n","- We have a whole dataset of **<span style='color:#F1A424'>unlabelled image</span>** data in folder test\n","- The unique ids of each image in the dataset are located in file `sample_submission.csv`\n","- Like for thr training dataset, well create a data loader, using only tensor transformation\n","- As we have no label data, we need a slightly modified data class"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-05-30T18:48:35.471966Z","iopub.status.busy":"2024-05-30T18:48:35.471556Z","iopub.status.idle":"2024-05-30T18:48:35.483097Z","shell.execute_reply":"2024-05-30T18:48:35.481991Z","shell.execute_reply.started":"2024-05-30T18:48:35.471936Z"},"trusted":true},"outputs":[],"source":["class pytorchdata_test(Dataset):\n","    \n","    def __init__(self, data_dir, transform,data_type=\"train\"):\n","        \n","        path2data = os.path.join(data_dir,data_type)\n","        filenames = os.listdir(path2data)\n","        self.full_filenames = [os.path.join(path2data, f) for f in filenames]\n","        \n","        # labels are in a csv file named train_labels.csv\n","        csv_filename=\"sample_submission.csv\"\n","        path2csvLabels=os.path.join(data_dir,csv_filename)\n","        labels_df=pd.read_csv(path2csvLabels)\n","        \n","        # set data frame index to id\n","        labels_df.set_index(\"id\", inplace=True)\n","        \n","        # obtain labels from data frame\n","        self.labels = [labels_df.loc[filename[:-4]].values[0] for filename in filenames]\n","        self.transform = transform       \n","        \n","    def __len__(self):\n","        # return size of dataset\n","        return len(self.full_filenames)\n","    \n","    def __getitem__(self, idx):\n","        # open image, apply transforms and return with label\n","        image = Image.open(self.full_filenames[idx]) # PIL image\n","        image = self.transform(image)\n","        return image, self.labels[idx]"]},{"cell_type":"markdown","metadata":{},"source":["#### **<span style='color:#F1A424'>CHECKS</span>**\n","\n","- Confirm our best performing model has been saved in the working directory \n","- Confirm the test folder contents are present"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-05-30T18:48:38.727039Z","iopub.status.busy":"2024-05-30T18:48:38.726619Z","iopub.status.idle":"2024-05-30T18:48:39.866412Z","shell.execute_reply":"2024-05-30T18:48:39.864839Z","shell.execute_reply.started":"2024-05-30T18:48:38.727006Z"},"trusted":true},"outputs":[],"source":["!ls"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-05-30T18:48:43.646392Z","iopub.status.busy":"2024-05-30T18:48:43.645581Z","iopub.status.idle":"2024-05-30T18:48:50.265161Z","shell.execute_reply":"2024-05-30T18:48:50.263799Z","shell.execute_reply.started":"2024-05-30T18:48:43.646332Z"},"trusted":true},"outputs":[],"source":["!ls '/kaggle/input/histopathologic-cancer-detection/test' | head -n 5"]},{"cell_type":"markdown","metadata":{},"source":["#### **<span style='color:#F1A424'>FUNCTION PARAMETER DICTIONARY</span>**\n","\n","Having defined a model architecture, we can load model weights"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-05-30T18:48:50.269657Z","iopub.status.busy":"2024-05-30T18:48:50.268672Z","iopub.status.idle":"2024-05-30T18:48:50.281798Z","shell.execute_reply":"2024-05-30T18:48:50.280712Z","shell.execute_reply.started":"2024-05-30T18:48:50.269610Z"},"trusted":true},"outputs":[],"source":["# load any model weights for the model\n","cnn_model.load_state_dict(torch.load('weights.pt'))"]},{"cell_type":"markdown","metadata":{},"source":["#### **<span style='color:#F1A424'> TEST FILE IDS</span>**\n","\n","The submission file contains all the ids to the files that are located in the test folder"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-05-30T18:48:50.283351Z","iopub.status.busy":"2024-05-30T18:48:50.283004Z","iopub.status.idle":"2024-05-30T18:48:50.413600Z","shell.execute_reply":"2024-05-30T18:48:50.412456Z","shell.execute_reply.started":"2024-05-30T18:48:50.283323Z"},"trusted":true},"outputs":[],"source":["# sample submission\n","path_sub = \"/kaggle/input/histopathologic-cancer-detection/sample_submission.csv\"\n","labels_df = pd.read_csv(path_sub)\n","labels_df.head()\n","labels_df.shape"]},{"cell_type":"markdown","metadata":{},"source":["#### **<span style='color:#F1A424'>TEST IMAGE DATASET</span>**\n","\n","Like we did with the training set, lets convert and store all image data in "]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-05-30T18:48:53.054251Z","iopub.status.busy":"2024-05-30T18:48:53.053844Z","iopub.status.idle":"2024-05-30T18:48:55.710203Z","shell.execute_reply":"2024-05-30T18:48:55.709001Z","shell.execute_reply.started":"2024-05-30T18:48:53.054224Z"},"trusted":true},"outputs":[],"source":["data_dir = '/kaggle/input/histopathologic-cancer-detection/'\n","\n","data_transformer = transforms.Compose([transforms.ToTensor(),\n","                                       transforms.Resize((46,46))])\n","\n","img_dataset_test = pytorchdata_test(data_dir,data_transformer,data_type=\"test\")\n","print(len(img_dataset_test), 'samples found')"]},{"cell_type":"markdown","metadata":{},"source":["#### **<span style='color:#F1A424'>PREDICTION FUNCTION</span>**\n","\n","For inference, we need to set the model to `torch.no_grad`\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-05-30T18:48:55.712704Z","iopub.status.busy":"2024-05-30T18:48:55.712353Z","iopub.status.idle":"2024-05-30T18:48:55.720004Z","shell.execute_reply":"2024-05-30T18:48:55.719136Z","shell.execute_reply.started":"2024-05-30T18:48:55.712675Z"},"trusted":true},"outputs":[],"source":["def inference(model,dataset,device,num_classes=2):\n","    \n","    len_data=len(dataset)\n","    y_out=torch.zeros(len_data,num_classes) # initialize output tensor on CPU\n","    y_gt=np.zeros((len_data),dtype=\"uint8\") # initialize ground truth on CPU\n","    model=model.to(device) # move model to device\n","    \n","    with torch.no_grad():\n","        for i in tqdm(range(len_data)):\n","            x,y=dataset[i]\n","            y_gt[i]=y\n","            y_out[i]=model(x.unsqueeze(0).to(device))\n","\n","    return y_out.numpy(),y_gt            "]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-05-30T18:48:55.808684Z","iopub.status.busy":"2024-05-30T18:48:55.805925Z","iopub.status.idle":"2024-05-30T18:49:51.825750Z","shell.execute_reply":"2024-05-30T18:49:51.824150Z","shell.execute_reply.started":"2024-05-30T18:48:55.808645Z"},"trusted":true},"outputs":[],"source":["y_test_out,_ = inference(cnn_model,img_dataset_test, device)            "]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2024-05-30T18:49:51.827248Z","iopub.status.idle":"2024-05-30T18:49:51.827926Z","shell.execute_reply":"2024-05-30T18:49:51.827743Z","shell.execute_reply.started":"2024-05-30T18:49:51.827723Z"},"trusted":true},"outputs":[],"source":["# class predictions 0,1\n","y_test_pred=np.argmax(y_test_out,axis=1)\n","print(y_test_pred.shape)\n","print(y_test_pred[0:5])"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2024-05-26T20:58:01.999868Z","iopub.status.idle":"2024-05-26T20:58:02.000209Z","shell.execute_reply":"2024-05-26T20:58:02.000060Z","shell.execute_reply.started":"2024-05-26T20:58:02.000041Z"},"trusted":true},"outputs":[],"source":["# probabilities of predicted selection\n","# return F.log_softmax(x, dim=1) ie.\n","preds = np.exp(y_test_out[:, 1])\n","print(preds.shape)\n","print(preds[0:5])"]}],"metadata":{"kaggle":{"accelerator":"none","dataSources":[{"databundleVersionId":862157,"sourceId":11848,"sourceType":"competition"}],"isGpuEnabled":false,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.13"}},"nbformat":4,"nbformat_minor":4}
