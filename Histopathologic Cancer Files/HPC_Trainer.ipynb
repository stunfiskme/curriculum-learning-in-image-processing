{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":11848,"databundleVersionId":862157,"sourceType":"competition"}],"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nfrom plotly.subplots import make_subplots\nimport plotly.graph_objs as go\nimport copy\nimport os\nimport torch\nfrom PIL import Image\nfrom PIL import Image, ImageDraw\nfrom torch.utils.data import Dataset, Sampler\nimport torchvision.transforms as transforms\nfrom torch.utils.data import random_split\nfrom torch.optim.lr_scheduler import ReduceLROnPlateau\nimport torch.nn as nn\nfrom torchvision import utils\nimport seaborn as sns; sns.set(style='whitegrid')\n%matplotlib inline","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_kg_hide-input":false,"execution":{"iopub.status.busy":"2024-06-04T23:00:17.051605Z","iopub.execute_input":"2024-06-04T23:00:17.052749Z","iopub.status.idle":"2024-06-04T23:00:24.628227Z","shell.execute_reply.started":"2024-06-04T23:00:17.052676Z","shell.execute_reply":"2024-06-04T23:00:24.627085Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"# library which allows us to view model summary like keras/tf\n!pip install torchsummary","metadata":{"execution":{"iopub.status.busy":"2024-06-04T23:00:24.630771Z","iopub.execute_input":"2024-06-04T23:00:24.631437Z","iopub.status.idle":"2024-06-04T23:00:38.758141Z","shell.execute_reply.started":"2024-06-04T23:00:24.631394Z","shell.execute_reply":"2024-06-04T23:00:38.756804Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"Collecting torchsummary\n  Downloading torchsummary-1.5.1-py3-none-any.whl.metadata (296 bytes)\nDownloading torchsummary-1.5.1-py3-none-any.whl (2.8 kB)\nInstalling collected packages: torchsummary\nSuccessfully installed torchsummary-1.5.1\n","output_type":"stream"}]},{"cell_type":"code","source":"labels_df = pd.read_csv('/kaggle/input/histopathologic-cancer-detection/train_labels.csv')\nprint(labels_df.head().to_markdown())\nos.listdir('/kaggle/input/histopathologic-cancer-detection/')\nlabels_df.shape\n# No duplicate ids found\nlabels_df[labels_df.duplicated(keep=False)]\nlabels_df['label'].value_counts()","metadata":{"execution":{"iopub.status.busy":"2024-06-04T23:00:38.764029Z","iopub.execute_input":"2024-06-04T23:00:38.764438Z","iopub.status.idle":"2024-06-04T23:00:39.318259Z","shell.execute_reply.started":"2024-06-04T23:00:38.764393Z","shell.execute_reply":"2024-06-04T23:00:39.317165Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stdout","text":"|    | id                                       |   label |\n|---:|:-----------------------------------------|--------:|\n|  0 | f38a6374c348f90b587e046aac6079959adf3835 |       0 |\n|  1 | c18f2d887b7ae4f6742ee445113fa1aef383ed77 |       1 |\n|  2 | 755db6279dae599ebb4d39a9123cce439965282d |       0 |\n|  3 | bc3f0c64fb968ff4a8bd33af6971ecae77c75e08 |       0 |\n|  4 | 068aba587a4950175d04c680d38943fd488d6a9d |       0 |\n","output_type":"stream"},{"execution_count":3,"output_type":"execute_result","data":{"text/plain":"label\n0    130908\n1     89117\nName: count, dtype: int64"},"metadata":{}}]},{"cell_type":"code","source":"torch.manual_seed(42) # fix random seed\n\nclass pytorch_data(Dataset):\n    \n    def __init__(self,data_dir,transform,data_type=\"train\"):      \n    \n        # Get Image File Names\n        cdm_data=os.path.join(data_dir,data_type)  # directory of files\n        \n        file_names = os.listdir(cdm_data) # get list of images in that directory  \n        idx_choose = np.random.choice(np.arange(len(file_names)), \n                                      4000,\n                                      replace=False).tolist()\n        file_names_sample = [file_names[x] for x in idx_choose]\n        self.full_filenames = [os.path.join(cdm_data, f) for f in file_names_sample]   # get the full path to images\n        \n        # Get Labels\n        labels_data=os.path.join(data_dir,\"train_labels.csv\") \n        labels_df=pd.read_csv(labels_data)\n        labels_df.set_index(\"id\", inplace=True) # set data frame index to id\n        self.labels = [labels_df.loc[filename[:-4]].values[0] for filename in file_names_sample]  # obtained labels from df\n        self.transform = transform\n      \n    def __len__(self):\n        return len(self.full_filenames) # size of dataset\n      \n    def __getitem__(self, idx):\n        # open image, apply transforms and return with label\n        image = Image.open(self.full_filenames[idx])  # Open Image with PIL\n        image = self.transform(image) # Apply Specific Transformation to Image\n        return image, self.labels[idx]\n","metadata":{"execution":{"iopub.status.busy":"2024-06-04T23:00:39.321915Z","iopub.execute_input":"2024-06-04T23:00:39.322609Z","iopub.status.idle":"2024-06-04T23:00:39.336895Z","shell.execute_reply.started":"2024-06-04T23:00:39.322569Z","shell.execute_reply":"2024-06-04T23:00:39.335625Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"# define transformation that converts a PIL image into PyTorch tensors\nimport torchvision.transforms as transforms\ndata_transformer = transforms.Compose([transforms.ToTensor(),\n                                       transforms.Resize((46,46))])","metadata":{"execution":{"iopub.status.busy":"2024-06-04T23:00:39.338492Z","iopub.execute_input":"2024-06-04T23:00:39.338967Z","iopub.status.idle":"2024-06-04T23:00:39.350213Z","shell.execute_reply.started":"2024-06-04T23:00:39.338926Z","shell.execute_reply":"2024-06-04T23:00:39.349031Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"# Define an object of the custom dataset for the train folder.\ndata_dir = '/kaggle/input/histopathologic-cancer-detection/'\nimg_dataset = pytorch_data(data_dir, data_transformer, \"train\") # Histopathalogic images","metadata":{"execution":{"iopub.status.busy":"2024-06-04T23:00:39.351717Z","iopub.execute_input":"2024-06-04T23:00:39.352138Z","iopub.status.idle":"2024-06-04T23:00:48.381270Z","shell.execute_reply.started":"2024-06-04T23:00:39.352103Z","shell.execute_reply":"2024-06-04T23:00:48.379848Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"# load an example tensor\nimg,label=img_dataset[10]\nprint(img.shape,torch.min(img),torch.max(img))","metadata":{"execution":{"iopub.status.busy":"2024-06-04T23:00:48.383702Z","iopub.execute_input":"2024-06-04T23:00:48.384231Z","iopub.status.idle":"2024-06-04T23:00:48.623961Z","shell.execute_reply.started":"2024-06-04T23:00:48.384187Z","shell.execute_reply":"2024-06-04T23:00:48.622725Z"},"trusted":true},"execution_count":7,"outputs":[{"name":"stdout","text":"torch.Size([3, 46, 46]) tensor(0.2142) tensor(0.9954)\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n  warnings.warn(\n","output_type":"stream"}]},{"cell_type":"code","source":"len_img=len(img_dataset)\nlen_train=int(0.8*len_img)\nlen_val=len_img-len_train\n\n# Split Pytorch tensor\ntrain_ts,val_ts=random_split(img_dataset,\n                             [len_train,len_val]) # random split 80/20\n\nprint(\"train dataset size:\", len(train_ts))\nprint(\"validation dataset size:\", len(val_ts))","metadata":{"execution":{"iopub.status.busy":"2024-06-04T23:00:48.625454Z","iopub.execute_input":"2024-06-04T23:00:48.625926Z","iopub.status.idle":"2024-06-04T23:00:48.638320Z","shell.execute_reply.started":"2024-06-04T23:00:48.625883Z","shell.execute_reply":"2024-06-04T23:00:48.637032Z"},"trusted":true},"execution_count":8,"outputs":[{"name":"stdout","text":"train dataset size: 3200\nvalidation dataset size: 800\n","output_type":"stream"}]},{"cell_type":"code","source":"import plotly.express as px\n\ndef plot_img(x,y,title=None):\n\n    npimg = x.numpy() # convert tensor to numpy array\n    npimg_tr=np.transpose(npimg, (1,2,0)) # Convert to H*W*C shape\n    fig = px.imshow(npimg_tr)\n    fig.update_layout(template='plotly_white')\n    fig.update_layout(title=title,height=300,margin={'l':10,'r':20,'b':10})\n    fig.show()","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2024-06-04T23:00:48.640475Z","iopub.execute_input":"2024-06-04T23:00:48.641045Z","iopub.status.idle":"2024-06-04T23:00:49.144295Z","shell.execute_reply.started":"2024-06-04T23:00:48.641005Z","shell.execute_reply":"2024-06-04T23:00:49.143286Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"# Define the following transformations for the training dataset\ntr_transf = transforms.Compose([\n#     transforms.Resize((40,40)),\n    transforms.RandomHorizontalFlip(p=0.5), \n    transforms.RandomVerticalFlip(p=0.5),  \n    transforms.RandomRotation(45),         \n#     transforms.RandomResizedCrop(50,scale=(0.8,1.0),ratio=(1.0,1.0)),\n    transforms.ToTensor()])","metadata":{"execution":{"iopub.status.busy":"2024-06-04T23:00:49.145779Z","iopub.execute_input":"2024-06-04T23:00:49.146129Z","iopub.status.idle":"2024-06-04T23:00:49.152433Z","shell.execute_reply.started":"2024-06-04T23:00:49.146100Z","shell.execute_reply":"2024-06-04T23:00:49.150778Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"# For the validation dataset, we don't need any augmentation; simply convert images into tensors\nval_transf = transforms.Compose([\n    transforms.ToTensor()])\n\n# After defining the transformations, overwrite the transform functions of train_ts, val_ts\ntrain_ts.transform=tr_transf\nval_ts.transform=val_transf","metadata":{"execution":{"iopub.status.busy":"2024-06-04T23:00:49.153973Z","iopub.execute_input":"2024-06-04T23:00:49.154474Z","iopub.status.idle":"2024-06-04T23:00:49.165813Z","shell.execute_reply.started":"2024-06-04T23:00:49.154434Z","shell.execute_reply":"2024-06-04T23:00:49.164764Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"# The subset can also have transform attribute (if we asign)\ntrain_ts.transform","metadata":{"execution":{"iopub.status.busy":"2024-06-04T23:00:49.166988Z","iopub.execute_input":"2024-06-04T23:00:49.167354Z","iopub.status.idle":"2024-06-04T23:00:49.181829Z","shell.execute_reply.started":"2024-06-04T23:00:49.167327Z","shell.execute_reply":"2024-06-04T23:00:49.180703Z"},"trusted":true},"execution_count":12,"outputs":[{"execution_count":12,"output_type":"execute_result","data":{"text/plain":"Compose(\n    RandomHorizontalFlip(p=0.5)\n    RandomVerticalFlip(p=0.5)\n    RandomRotation(degrees=[-45.0, 45.0], interpolation=nearest, expand=False, fill=0)\n    ToTensor()\n)"},"metadata":{}}]},{"cell_type":"code","source":"import pywt\n#Sorter\nclass RMSESampler(Sampler[int]):\n    \n    def __init__(self, data: torch.utils.data.Subset) -> None:\n        self.data = data\n        \n    def __len__(self) -> int:\n        return len(self.data)\n    \n    def __iter__(self): # -> iter[int]:\n        images = []\n        for i in self.data.indices:\n            images.append(self.data.dataset.__getitem__(i))\n        rmse = []\n        for img in images:\n            img_array = img[0].numpy().copy()\n            img_shape = img_array.shape\n            img_resize = np.resize(np.resize(img_array, (img_shape[0] // 2, img_shape[1] // 2, img_shape[2] // 2)),\n                (img_shape[0], img_shape[1], img_shape[2])\n            )\n            img_resize_array = np.asarray(img_resize)\n\n            # RMSE\n            # Get the RMSE between the original and the resized version\n            img_rmse = np.sqrt(np.mean((img_array - img_resize_array) ** 2))\n            rmse.append(img_rmse)\n        rmse = np.negative(np.abs(rmse - np.mean(rmse)))\n        rmse = torch.from_numpy(np.asarray(rmse))\n        yield from torch.argsort(rmse).tolist()\n        \nclass WDSampler(Sampler[int]):\n    \n    def __init__(self, data: torch.utils.data.Subset) -> None:\n        self.data = data\n        \n    def __len__(self) -> int:\n        return len(self.data)\n    \n    def __iter__(self): # -> iter[int]:\n        images = []\n        for i in self.data.indices:\n            images.append(self.data.dataset.__getitem__(i))\n        decomp = []\n        for img in images:\n            img_array = img[0].numpy().copy()\n            wd = pywt.wavedec2(img_array, \"db2\", level=1)\n            decomp.append(np.sum((np.square(wd[1][0]), np.square(wd[1][1]), np.square(wd[1][2]))))\n#         decomp = np.abs(decomp - np.mean(decomp))\n#         decomp = np.negative(np.abs(decomp - np.mean(decomp)))\n#         decomp = torch.from_numpy(np.array(decomp))\n        decomp = torch.from_numpy(np.negative(np.array(decomp)))\n        yield from torch.argsort(decomp).tolist()\n        \nclass RMSEBatchSampler(Sampler[list[int]]):\n    \n    def __init__(self, data: list[str], batch_size: int) -> None:\n        self.data = data\n        self.batch_size = batch_size\n        \n    def __len__(self) -> int:\n        return (len(self.data) + self.batch_size - 1) // self.batch_size\n    \n    def __iter__(self):\n        images = []\n        for i in self.data.indices:\n            images.append(self.data.dataset.__getitem__(i))\n        rmse = []\n        for img in images:\n            img_array = img[0].numpy().copy()\n            img_shape = img_array.shape\n            img_resize = np.resize(np.resize(img_array, (img_shape[0] // 2, img_shape[1] // 2, img_shape[2] // 2)),\n                (img_shape[0], img_shape[1], img_shape[2])\n            )\n            img_resize_array = np.asarray(img_resize)\n\n            # RMSE\n            # Get the RMSE between the original and the resized version\n            img_rmse = np.sqrt(np.mean((img_array - img_resize_array) ** 2))\n            rmse.append(img_rmse)\n        rmse = np.negative(np.abs(rmse - np.mean(rmse)))\n        rmse = torch.from_numpy(rmse)\n        for batch in torch.chunk(torch.argsort(rmse), len(self)):\n            yield batch.tolist()\n            \nclass WDBatchSampler(Sampler[list[int]]):\n    \n    def __init__(self, data: list[str], batch_size: int) -> None:\n        self.data = data\n        self.batch_size = batch_size\n        \n    def __len__(self) -> int:\n        return (len(self.data) + self.batch_size - 1) // self.batch_size\n    \n    def __iter__(self):\n        images = []\n        for i in self.data.indices:\n            images.append(self.data.dataset.__getitem__(i))\n        decomp = []\n        for img in images:\n            img_array = img[0].numpy().copy()\n            wd = pywt.wavedec2(img_array, \"db2\", level=3)\n            decomp.append(np.sum((np.square(wd[3][0]), np.square(wd[3][1]), np.square(wd[3][2])))) #np.square(wd[1][0]), np.square(wd[1][1]), np.square(wd[1][2])\n#         decomp = np.abs(decomp - np.mean(decomp))\n        decomp = np.negative(np.abs(decomp - np.mean(decomp)))\n#         decomp = torch.from_numpy(np.array(decomp))\n#         decomp = torch.from_numpy(np.negative(np.array(decomp)))\n        for batch in torch.chunk(torch.argsort(torch.from_numpy(decomp)), len(self)):\n            yield batch.tolist()","metadata":{"execution":{"iopub.status.busy":"2024-06-04T23:00:49.186220Z","iopub.execute_input":"2024-06-04T23:00:49.186681Z","iopub.status.idle":"2024-06-04T23:00:49.292583Z","shell.execute_reply.started":"2024-06-04T23:00:49.186649Z","shell.execute_reply":"2024-06-04T23:00:49.291424Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"from torch.utils.data import DataLoader\n\n# # Training DataLoader\n# train_dl = DataLoader(train_ts, \n#                       shuffle=False,\n#                       batch_sampler=WTFBatchSampler(train_ts, 16))\n\n# Training DataLoader\ntrain_dl = DataLoader(train_ts,\n                      batch_size=16, \n                      shuffle=False)\n\n# Validation DataLoader\nval_dl = DataLoader(val_ts,\n                    batch_size=16,\n                    shuffle=False)","metadata":{"execution":{"iopub.status.busy":"2024-06-04T23:00:49.294083Z","iopub.execute_input":"2024-06-04T23:00:49.295038Z","iopub.status.idle":"2024-06-04T23:00:49.302220Z","shell.execute_reply.started":"2024-06-04T23:00:49.294989Z","shell.execute_reply":"2024-06-04T23:00:49.300870Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"code","source":"# check samples\nfor x,y in train_dl:\n    print(x.shape,y)\n    break","metadata":{"execution":{"iopub.status.busy":"2024-06-04T23:00:49.304016Z","iopub.execute_input":"2024-06-04T23:00:49.304501Z","iopub.status.idle":"2024-06-04T23:00:49.475824Z","shell.execute_reply.started":"2024-06-04T23:00:49.304456Z","shell.execute_reply":"2024-06-04T23:00:49.474765Z"},"trusted":true},"execution_count":15,"outputs":[{"name":"stdout","text":"torch.Size([16, 3, 46, 46]) tensor([0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0])\n","output_type":"stream"}]},{"cell_type":"code","source":"def findConv2dOutShape(hin,win,conv,pool=2):\n    # get conv arguments\n    kernel_size=conv.kernel_size\n    stride=conv.stride\n    padding=conv.padding\n    dilation=conv.dilation\n\n    hout=np.floor((hin+2*padding[0]-dilation[0]*(kernel_size[0]-1)-1)/stride[0]+1)\n    wout=np.floor((win+2*padding[1]-dilation[1]*(kernel_size[1]-1)-1)/stride[1]+1)\n\n    if pool:\n        hout/=pool\n        wout/=pool\n    return int(hout),int(wout)\n\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n# Neural Network\nclass Network(nn.Module):\n    \n    # Network Initialisation\n    def __init__(self, params):\n        \n        super(Network, self).__init__()\n    \n        Cin,Hin,Win=params[\"shape_in\"]\n        init_f=params[\"initial_filters\"] \n        num_fc1=params[\"num_fc1\"]  \n        num_classes=params[\"num_classes\"] \n        self.dropout_rate=params[\"dropout_rate\"] \n        \n        # Convolution Layers\n        self.conv1 = nn.Conv2d(Cin, init_f, kernel_size=3)\n        h,w=findConv2dOutShape(Hin,Win,self.conv1)\n        self.conv2 = nn.Conv2d(init_f, 2*init_f, kernel_size=3)\n        h,w=findConv2dOutShape(h,w,self.conv2)\n        self.conv3 = nn.Conv2d(2*init_f, 4*init_f, kernel_size=3)\n        h,w=findConv2dOutShape(h,w,self.conv3)\n        self.conv4 = nn.Conv2d(4*init_f, 8*init_f, kernel_size=3)\n        h,w=findConv2dOutShape(h,w,self.conv4)\n        \n        # compute the flatten size\n        self.num_flatten=h*w*8*init_f\n        self.fc1 = nn.Linear(self.num_flatten, num_fc1)\n        self.fc2 = nn.Linear(num_fc1, num_classes)\n\n    def forward(self,X):\n        \n        # Convolution & Pool Layers\n        X = F.relu(self.conv1(X)); \n        X = F.max_pool2d(X, 2, 2)\n        X = F.relu(self.conv2(X))\n        X = F.max_pool2d(X, 2, 2)\n        X = F.relu(self.conv3(X))\n        X = F.max_pool2d(X, 2, 2)\n        X = F.relu(self.conv4(X))\n        X = F.max_pool2d(X, 2, 2)\n\n        X = X.view(-1, self.num_flatten)\n        \n        X = F.relu(self.fc1(X))\n        X=F.dropout(X, self.dropout_rate)\n        X = self.fc2(X)\n        return F.log_softmax(X, dim=1)","metadata":{"_kg_hide-input":false,"execution":{"iopub.status.busy":"2024-06-04T23:00:49.477389Z","iopub.execute_input":"2024-06-04T23:00:49.477786Z","iopub.status.idle":"2024-06-04T23:00:49.491961Z","shell.execute_reply.started":"2024-06-04T23:00:49.477755Z","shell.execute_reply":"2024-06-04T23:00:49.490746Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"code","source":"# Neural Network Predefined Parameters\nparams_model={\n        \"shape_in\": (3,46,46), \n        \"initial_filters\": 8,    \n        \"num_fc1\": 100,\n        \"dropout_rate\": 0.25,\n        \"num_classes\": 2}\n\n# Create instantiation of Network class\ncnn_model = Network(params_model)\n\n# define computation hardware approach (GPU/CPU)\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nmodel = cnn_model.to(device)","metadata":{"execution":{"iopub.status.busy":"2024-06-04T23:00:49.493234Z","iopub.execute_input":"2024-06-04T23:00:49.493566Z","iopub.status.idle":"2024-06-04T23:00:49.522619Z","shell.execute_reply.started":"2024-06-04T23:00:49.493539Z","shell.execute_reply":"2024-06-04T23:00:49.521399Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"code","source":"from torchsummary import summary\nsummary(cnn_model, input_size=(3, 46, 46),device=device.type)","metadata":{"execution":{"iopub.status.busy":"2024-06-04T23:00:49.524041Z","iopub.execute_input":"2024-06-04T23:00:49.524410Z","iopub.status.idle":"2024-06-04T23:00:49.636092Z","shell.execute_reply.started":"2024-06-04T23:00:49.524380Z","shell.execute_reply":"2024-06-04T23:00:49.635004Z"},"trusted":true},"execution_count":18,"outputs":[{"name":"stdout","text":"----------------------------------------------------------------\n        Layer (type)               Output Shape         Param #\n================================================================\n            Conv2d-1            [-1, 8, 44, 44]             224\n            Conv2d-2           [-1, 16, 20, 20]           1,168\n            Conv2d-3             [-1, 32, 8, 8]           4,640\n            Conv2d-4             [-1, 64, 2, 2]          18,496\n            Linear-5                  [-1, 100]           6,500\n            Linear-6                    [-1, 2]             202\n================================================================\nTotal params: 31,230\nTrainable params: 31,230\nNon-trainable params: 0\n----------------------------------------------------------------\nInput size (MB): 0.02\nForward/backward pass size (MB): 0.19\nParams size (MB): 0.12\nEstimated Total Size (MB): 0.33\n----------------------------------------------------------------\n","output_type":"stream"}]},{"cell_type":"code","source":"loss_func = nn.NLLLoss(reduction=\"sum\")","metadata":{"execution":{"iopub.status.busy":"2024-06-04T23:00:49.637394Z","iopub.execute_input":"2024-06-04T23:00:49.637770Z","iopub.status.idle":"2024-06-04T23:00:49.643019Z","shell.execute_reply.started":"2024-06-04T23:00:49.637739Z","shell.execute_reply":"2024-06-04T23:00:49.641752Z"},"trusted":true},"execution_count":19,"outputs":[]},{"cell_type":"code","source":"from torch import optim\nopt = optim.Adam(cnn_model.parameters(), lr=3e-4)\nlr_scheduler = ReduceLROnPlateau(opt, mode='min',factor=0.5, patience=20,verbose=1)","metadata":{"execution":{"iopub.status.busy":"2024-06-04T23:00:49.644380Z","iopub.execute_input":"2024-06-04T23:00:49.644831Z","iopub.status.idle":"2024-06-04T23:00:49.654864Z","shell.execute_reply.started":"2024-06-04T23:00:49.644792Z","shell.execute_reply":"2024-06-04T23:00:49.653777Z"},"trusted":true},"execution_count":20,"outputs":[]},{"cell_type":"code","source":"''' Helper Functions'''\n\n        \n# Function to get the learning rate\ndef get_lr(opt):\n    for param_group in opt.param_groups:\n        return param_group['lr']\n\n# Function to compute the loss value per batch of data\ndef loss_batch(loss_func, output, target, opt=None):\n    \n    loss = loss_func(output, target) # get loss\n    pred = output.argmax(dim=1, keepdim=True) # Get Output Class\n    metric_b=pred.eq(target.view_as(pred)).sum().item() # get performance metric\n    \n    if opt is not None:\n        opt.zero_grad()\n        loss.backward()\n        opt.step()\n\n    return loss.item(), metric_b\n\n# Compute the loss value & performance metric for the entire dataset (epoch)\ndef loss_epoch(model,loss_func,dataset_dl,opt=None):\n    \n    run_loss=0.0 \n    t_metric=0.0\n    len_data=len(dataset_dl.dataset)\n\n    loss_stats = []\n    \n    # internal loop over dataset\n    for xb, yb in dataset_dl:\n        # move batch to device\n        xb=xb.to(device)\n        yb=yb.to(device)\n        output=model(xb) # get model output\n        loss_b,metric_b=loss_batch(loss_func, output, yb, opt) # get loss per batch\n        run_loss+=loss_b        # update running loss\n            \n        if metric_b is not None: # update running metric\n            t_metric+=metric_b    \n    \n    loss=run_loss/float(len_data)  # average loss value\n    metric=t_metric/float(len_data) # average metric value\n    \n    return loss, metric","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2024-06-04T23:00:49.656115Z","iopub.execute_input":"2024-06-04T23:00:49.656493Z","iopub.status.idle":"2024-06-04T23:00:49.668978Z","shell.execute_reply.started":"2024-06-04T23:00:49.656460Z","shell.execute_reply":"2024-06-04T23:00:49.667647Z"},"trusted":true},"execution_count":21,"outputs":[]},{"cell_type":"code","source":"from tqdm.notebook import trange, tqdm\n\ndef train_val(model, params,verbose=False):\n    \n    # Get the parameters\n    epochs=params[\"epochs\"]\n    loss_func=params[\"f_loss\"]\n    opt=params[\"optimiser\"]\n    train_dl=params[\"train\"]\n    val_dl=params[\"val\"]\n    lr_scheduler=params[\"lr_change\"]\n    weight_path=params[\"weight_path\"]\n    \n    loss_history={\"train\": [],\"val\": []} # history of loss values in each epoch\n    metric_history={\"train\": [],\"val\": []} # histroy of metric values in each epoch\n    best_model_wts = copy.deepcopy(model.state_dict()) # a deep copy of weights for the best performing model\n    best_loss=float('inf') # initialize best loss to a large value\n    \n    ''' Train Model n_epochs '''\n    \n    for epoch in tqdm(range(epochs)):\n        \n        ''' Get the Learning Rate '''\n        current_lr=get_lr(opt)\n        if(verbose):\n            print('Epoch {}/{}, current lr={}'.format(epoch, epochs - 1, current_lr))\n        \n        '''\n        \n        Train Model Process\n        \n        '''\n        \n        model.train()\n        train_loss, train_metric = loss_epoch(model,loss_func,train_dl,opt)\n\n        # collect losses\n        loss_history[\"train\"].append(train_loss)\n        metric_history[\"train\"].append(train_metric)\n        \n        '''\n        \n        Evaluate Model Process\n        \n        '''\n        \n        model.eval()\n        with torch.no_grad():\n            val_loss, val_metric = loss_epoch(model,loss_func,val_dl)\n        \n        # store best model\n        if(val_loss < best_loss):\n            best_loss = val_loss\n            best_model_wts = copy.deepcopy(model.state_dict())\n            \n            # store weights into a local file\n            torch.save(model.state_dict(), weight_path)\n            if(verbose):\n                print(\"Copied best model weights!\")\n        \n        # collect loss and metric for validation dataset\n        loss_history[\"val\"].append(val_loss)\n        metric_history[\"val\"].append(val_metric)\n        \n        # learning rate schedule\n        lr_scheduler.step(val_loss)\n        if current_lr != get_lr(opt):\n            if(verbose):\n                print(\"Loading best model weights!\")\n                model.load_state_dict(best_model_wts) \n\n        if(verbose):\n            print(f\"train loss: {train_loss:.6f}, dev loss: {val_loss:.6f}, accuracy: {100*val_metric:.2f}\")\n            print(\"-\"*10) \n\n    # load best model weights\n    model.load_state_dict(best_model_wts)\n        \n    return model, loss_history, metric_history","metadata":{"execution":{"iopub.status.busy":"2024-06-04T23:00:49.670961Z","iopub.execute_input":"2024-06-04T23:00:49.671394Z","iopub.status.idle":"2024-06-04T23:00:49.688569Z","shell.execute_reply.started":"2024-06-04T23:00:49.671355Z","shell.execute_reply":"2024-06-04T23:00:49.687420Z"},"trusted":true},"execution_count":22,"outputs":[]},{"cell_type":"code","source":"# params_train={\n#  \"train\": train_dl,\"val\": val_dl,\n#  \"epochs\": 50,\n#  \"optimiser\": optim.Adam(cnn_model.parameters(),lr=3e-4),\n#  \"lr_change\": ReduceLROnPlateau(opt,\n#                                 mode='min',\n#                                 factor=0.5,\n#                                 patience=20,\n#                                 verbose=1),\n#  \"f_loss\": nn.NLLLoss(reduction=\"sum\"),\n#  \"weight_path\": \"weights.pt\",\n# }\n\n# ''' Actual Train / Evaluation of CNN Model '''\n# # train and validate the model\n\n# cnn_model,loss_hist,metric_hist=train_val(cnn_model,params_train, True)","metadata":{"execution":{"iopub.status.busy":"2024-06-04T23:00:49.690157Z","iopub.execute_input":"2024-06-04T23:00:49.690618Z","iopub.status.idle":"2024-06-04T23:00:49.703269Z","shell.execute_reply.started":"2024-06-04T23:00:49.690578Z","shell.execute_reply":"2024-06-04T23:00:49.702239Z"},"trusted":true},"execution_count":23,"outputs":[]},{"cell_type":"code","source":"# import seaborn as sns; sns.set(style='whitegrid')\n\n# epochs=params_train[\"epochs\"]\n\n# fig,ax = plt.subplots(1,2,figsize=(12,5))\n\n# sns.lineplot(x=[*range(1,epochs+1)],y=loss_hist[\"train\"],ax=ax[0],label='loss_hist[\"train\"]')\n# sns.lineplot(x=[*range(1,epochs+1)],y=loss_hist[\"val\"],ax=ax[0],label='loss_hist[\"val\"]')\n# sns.lineplot(x=[*range(1,epochs+1)],y=metric_hist[\"train\"],ax=ax[1],label='metric_hist[\"train\"]')\n# sns.lineplot(x=[*range(1,epochs+1)],y=metric_hist[\"val\"],ax=ax[1],label='metric_hist[\"val\"]')\n# plt.title('Convergence History')","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2024-06-04T23:00:49.704846Z","iopub.execute_input":"2024-06-04T23:00:49.706665Z","iopub.status.idle":"2024-06-04T23:00:49.720180Z","shell.execute_reply.started":"2024-06-04T23:00:49.706623Z","shell.execute_reply":"2024-06-04T23:00:49.719058Z"},"trusted":true},"execution_count":24,"outputs":[]},{"cell_type":"code","source":"import warnings\nwarnings.filterwarnings(\"ignore\", \"is_categorical_dtype\")\nwarnings.filterwarnings(\"ignore\", \"use_inf_as_na\")","metadata":{"execution":{"iopub.status.busy":"2024-06-04T23:00:49.721746Z","iopub.execute_input":"2024-06-04T23:00:49.722268Z","iopub.status.idle":"2024-06-04T23:00:49.734124Z","shell.execute_reply.started":"2024-06-04T23:00:49.722228Z","shell.execute_reply":"2024-06-04T23:00:49.732932Z"},"trusted":true},"execution_count":25,"outputs":[]},{"cell_type":"code","source":"train_dataset_mean = [[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0]]\ntrain_dataset_std = [[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0]]\n\ntry:\n    cnn_model.load_state_dict(reset_weights)\nexcept:\n    reset_weights = copy.deepcopy(cnn_model.state_dict())\n    \n# sortings\ndef decCalcMiddle(decomp, mean):\n    return np.abs(np.subtract(decomp, mean))\ndef decCalcMiddleReversed(decomp, mean):\n    return np.negative(np.abs(np.subtract(decomp, mean)))\ndef decCalcOrder(decomp, mean):\n    return np.array(decomp)\ndef decCalcOrderReversed(decomp, mean):\n    return np.negative(np.array(decomp))\n\nsortings = [decCalcMiddle, decCalcOrder] # [decCalcMiddle, decCalcMiddleReversed, decCalcOrder, decCalcOrderReversed]\nsortnames = [\"from Middle\", \"in Order\"] #[\"from Middle\", \"from Middle, reversed\", \"in Order\", \"in Reverse Order\"]\n\n# sums\n\ndef sumOfLevels(wd, level, all_levels = False):\n    if level == 0:\n        return np.sum((np.square(wd)))\n    if all_levels:\n        sum_of_levels = 0\n        for l in range(level): #weird subtraction and adding here so that the approx doesn't get messed up\n            sum_of_levels+=np.sum((np.square(wd[l])))\n        return sum_of_levels\n    else:\n        return np.sum((np.square(wd[level])))\n\ndef horizontal(wd, level, all_levels = False):\n    if level == 0:\n        return np.sum((np.square(wd)))\n    if all_levels:\n        sum_of_levels = 0\n        for l in range(level-1):\n            if l == 0:\n                sum_of_levels+=np.square(wd[l])\n                continue\n            sum_of_levels+=np.square(wd[l][0])\n        return sum_of_levels\n    else:\n        return np.sum((np.square(wd[level][0])))\n    \ndef vertical(wd, level, all_levels = False):\n    if level == 0:\n        return np.sum((np.square(wd)))\n    if all_levels:\n        sum_of_levels = 0\n        for l in range(level-1):\n            if l == 0:\n                sum_of_levels+=np.square(wd[l])\n                continue\n            sum_of_levels+=np.square(wd[l][1])\n        return sum_of_levels\n    else:\n        return np.sum((np.square(wd[level][1])))\n    \ndef diagonal(wd, level, all_levels = False):\n    if level == 0:\n        return np.sum((np.square(wd)))\n    if all_levels:\n        sum_of_levels = 0\n        for l in range(level-1):\n            if l == 0:\n                sum_of_levels+=np.square(wd[l])\n                continue\n            sum_of_levels+=np.square(wd[l][2])\n        return sum_of_levels\n    else:\n        return np.sum((np.square(wd[level][2])))\n\nsums = [horizontal, vertical, diagonal]#[sumOfLevels, horizontal, vertical, diagonal]\nsumnames = [\"Horizontal\", \"Vertical\", \"Diagonal\"] #[\"All directions\", \"Horizontal\", \"Vertical\", \"Diagonal\"]\n\nreset_weights = copy.deepcopy(cnn_model.state_dict())\n\nimg_stats = []\n\n\n\nfor img_array in train_dl:\n    for img in img_array[0]:\n        img_stats.append(pywt.wavedec2(img, \"db2\", level=3))\n\nfor lev in range(1,4):\n    for direction in range(0,3):\n        train_dataset_mean[lev-1][direction] = np.round(np.mean(img_stats[:][lev][direction]), 2)\n        train_dataset_std[lev-1][direction] = np.round(np.std(img_stats[:][lev][direction]),2)\n    print(f\"\"\"TRAINING DATASET STATS FOR LEVEL {lev}:\\n\n    HORIZONTAL: Mean: {train_dataset_mean[lev-1][0]}\\tSt.d.: {train_dataset_std[lev-1][0]}\\n\n    VERTICAL:   Mean: {train_dataset_mean[lev-1][1]}\\tSt.d.: {train_dataset_std[lev-1][1]}\\n\n    DIAGONAL:   Mean: {train_dataset_mean[lev-1][2]}\\tSt.d.: {train_dataset_std[lev-1][2]}\\n\"\"\")\n            \nfor lev in tqdm(range(1,4)):\n    print(\"|||||||||||||||||||||||||||||||||||||||||||||||||||||||||\")\n    print(f\"LEVEL {lev}\")\n    for sort, sortname in tqdm(zip(sortings, sortnames)):\n        print(\"---------------------------------------------------------\")\n        print(f\"SORTING {sortname}\")\n        j=0\n        for summation, sumname in tqdm(zip(sums, sumnames)):\n            print(train_dataset_mean)\n            mean = train_dataset_mean[lev][j] # FHIX  \n            cnn_model.load_state_dict(reset_weights)\n            print(\".........................................................\")\n            print(f\"SUMMATION {sumname}\")\n\n            class WDSampler(Sampler[int]):\n\n                def __init__(self, data: torch.utils.data.Subset) -> None:\n                    self.data = data\n\n                def __len__(self) -> int:\n                    return len(self.data)\n\n                def __iter__(self): # -> iter[int]:\n                    images = []\n                    for i in self.data.indices:\n                        images.append(self.data.dataset.__getitem__(i))\n                    decomp = []\n                    for img in images:\n                        img_array = img[0].numpy().copy()\n                        wd = pywt.wavedec2(img_array, \"rbio2.2\", level=lev)\n                        decomp.append(summation(wd, lev))\n                    decomp = sort(decomp)\n                    yield from torch.argsort(torch.from_numpy(decomp)).tolist()\n\n            class WDBatchSampler(Sampler[list[int]]):\n\n                def __init__(self, data: list[str], batch_size: int) -> None:\n                    self.data = data\n                    self.batch_size = batch_size\n\n                def __len__(self) -> int:\n                    return (len(self.data) + self.batch_size - 1) // self.batch_size\n\n                def __iter__(self):\n                    images = []\n                    for i in self.data.indices:\n                        images.append(self.data.dataset.__getitem__(i))\n\n                    decomp = []\n                    for img in images:\n                        img_array = img[0].numpy().copy()\n                        wd = pywt.wavedec2(img_array, \"db2\", level=lev)\n                        decomp.append(summation(wd, lev))\n                    decomp = sort(decomp, train_dataset_mean[lev][j])\n                    for batch in torch.chunk(torch.argsort(torch.from_numpy(decomp)), len(self)):\n                        yield batch.tolist()\n\n            train_dl = DataLoader(train_ts, \n                                  shuffle=False,\n                                  batch_sampler=WDBatchSampler(train_ts, 16))\n\n#                 train_dl = DataLoader(train_ts, \n#                                       shuffle=False,\n#                                       batch_size=16,\n#                                       sampler=WDSampler(train_ts))\n\n#                 train_dl = DataLoader(train_ts, \n#                                       shuffle=False,\n#                                       batch_size=16)\n\n            params_train={\n             \"train\": train_dl,\"val\": val_dl,\n             \"epochs\": 10,\n             \"optimiser\": optim.Adam(cnn_model.parameters(),lr=3e-4),\n             \"lr_change\": ReduceLROnPlateau(opt,\n                                            mode='min',\n                                            factor=0.5,\n                                            patience=20,\n                                            verbose=1),\n             \"f_loss\": nn.NLLLoss(reduction=\"sum\"),\n             \"weight_path\": \"weights.pt\",\n            }\n\n            ''' Actual Train / Evaluation of CNN Model '''\n            # train and validate the model\n\n            cnn_model,loss_hist,metric_hist=train_val(cnn_model,params_train)\n\n            epochs=params_train[\"epochs\"]\n\n            fig,ax = plt.subplots(1,2,figsize=(12,5))\n\n            sns.lineplot(x=[*range(1,epochs+1)],y=loss_hist[\"train\"],ax=ax[0],label='loss_hist[\"train\"]')\n            sns.lineplot(x=[*range(1,epochs+1)],y=loss_hist[\"val\"],ax=ax[0],label='loss_hist[\"val\"]')\n            sns.lineplot(x=[*range(1,epochs+1)],y=metric_hist[\"train\"],ax=ax[1],label='metric_hist[\"train\"]')\n            sns.lineplot(x=[*range(1,epochs+1)],y=metric_hist[\"val\"],ax=ax[1],label='metric_hist[\"val\"]')\n            plt.title(f\"LEVEL {lev} {sortname} on {sumname}\")\n            j+=1","metadata":{"execution":{"iopub.status.busy":"2024-06-04T23:00:49.735853Z","iopub.execute_input":"2024-06-04T23:00:49.736241Z","iopub.status.idle":"2024-06-04T23:01:21.832726Z","shell.execute_reply.started":"2024-06-04T23:00:49.736209Z","shell.execute_reply":"2024-06-04T23:01:21.830781Z"},"trusted":true},"execution_count":26,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"TRAINING DATASET STATS FOR LEVEL 1:\n\n    HORIZONTAL: Mean: 3.049999952316284\tSt.d.: 0.8799999952316284\n\n    VERTICAL:   Mean: 0.03999999910593033\tSt.d.: 0.36000001430511475\n\n    DIAGONAL:   Mean: 0.009999999776482582\tSt.d.: 0.25\n\nTRAINING DATASET STATS FOR LEVEL 2:\n\n    HORIZONTAL: Mean: 4.070000171661377\tSt.d.: 1.2699999809265137\n\n    VERTICAL:   Mean: 0.009999999776482582\tSt.d.: 0.20999999344348907\n\n    DIAGONAL:   Mean: 0.0\tSt.d.: 0.14000000059604645\n\nTRAINING DATASET STATS FOR LEVEL 3:\n\n    HORIZONTAL: Mean: 4.28000020980835\tSt.d.: 0.8600000143051147\n\n    VERTICAL:   Mean: -0.0\tSt.d.: 0.25999999046325684\n\n    DIAGONAL:   Mean: 0.009999999776482582\tSt.d.: 0.20999999344348907\n\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/3 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"bf35fd0e442d4e328c3ac65d9501eb63"}},"metadata":{}},{"name":"stdout","text":"|||||||||||||||||||||||||||||||||||||||||||||||||||||||||\nLEVEL 1\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"0it [00:00, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ce7d3fec1a3541b79d56441b04840ffb"}},"metadata":{}},{"name":"stdout","text":"---------------------------------------------------------\nSORTING from Middle\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"0it [00:00, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b63343bb71e04f6da998ebcadd6cbf71"}},"metadata":{}},{"name":"stdout","text":"[[3.05, 0.04, 0.01], [4.07, 0.01, 0.0], [4.28, -0.0, 0.01]]\n.........................................................\nSUMMATION Horizontal\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/10 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"afcb9a2706df436a8e80320e5c6d39c5"}},"metadata":{}},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)","Cell \u001b[0;32mIn[26], line 185\u001b[0m\n\u001b[1;32m    182\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m''' Actual Train / Evaluation of CNN Model '''\u001b[39;00m\n\u001b[1;32m    183\u001b[0m \u001b[38;5;66;03m# train and validate the model\u001b[39;00m\n\u001b[0;32m--> 185\u001b[0m cnn_model,loss_hist,metric_hist\u001b[38;5;241m=\u001b[39m\u001b[43mtrain_val\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcnn_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43mparams_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    187\u001b[0m epochs\u001b[38;5;241m=\u001b[39mparams_train[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mepochs\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m    189\u001b[0m fig,ax \u001b[38;5;241m=\u001b[39m plt\u001b[38;5;241m.\u001b[39msubplots(\u001b[38;5;241m1\u001b[39m,\u001b[38;5;241m2\u001b[39m,figsize\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m12\u001b[39m,\u001b[38;5;241m5\u001b[39m))\n","Cell \u001b[0;32mIn[22], line 35\u001b[0m, in \u001b[0;36mtrain_val\u001b[0;34m(model, params, verbose)\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m'''\u001b[39;00m\n\u001b[1;32m     29\u001b[0m \u001b[38;5;124;03m\u001b[39;00m\n\u001b[1;32m     30\u001b[0m \u001b[38;5;124;03mTrain Model Process\u001b[39;00m\n\u001b[1;32m     31\u001b[0m \u001b[38;5;124;03m\u001b[39;00m\n\u001b[1;32m     32\u001b[0m \u001b[38;5;124;03m'''\u001b[39;00m\n\u001b[1;32m     34\u001b[0m model\u001b[38;5;241m.\u001b[39mtrain()\n\u001b[0;32m---> 35\u001b[0m train_loss, train_metric, mean_loss_complexity \u001b[38;5;241m=\u001b[39m \u001b[43mloss_epoch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43mloss_func\u001b[49m\u001b[43m,\u001b[49m\u001b[43mtrain_dl\u001b[49m\u001b[43m,\u001b[49m\u001b[43mopt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m     37\u001b[0m \u001b[38;5;66;03m# collect losses\u001b[39;00m\n\u001b[1;32m     38\u001b[0m loss_history[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mappend(train_loss)\n","\u001b[0;31mTypeError\u001b[0m: loss_epoch() takes from 3 to 4 positional arguments but 5 were given"],"ename":"TypeError","evalue":"loss_epoch() takes from 3 to 4 positional arguments but 5 were given","output_type":"error"}]},{"cell_type":"markdown","source":"## <b>11 <span style='color:#F1A424'>|</span> Inference</b> \n\n- Once we have trained our model using `train_val`, we can begin to utilise it to **<span style='color:#F1A424'>make some predictions</span>**\n- We have a whole dataset of **<span style='color:#F1A424'>unlabelled image</span>** data in folder test\n- The unique ids of each image in the dataset are located in file `sample_submission.csv`\n- Like for thr training dataset, well create a data loader, using only tensor transformation\n- As we have no label data, we need a slightly modified data class","metadata":{}},{"cell_type":"code","source":"class pytorchdata_test(Dataset):\n    \n    def __init__(self, data_dir, transform,data_type=\"train\"):\n        \n        path2data = os.path.join(data_dir,data_type)\n        filenames = os.listdir(path2data)\n        self.full_filenames = [os.path.join(path2data, f) for f in filenames]\n        \n        # labels are in a csv file named train_labels.csv\n        csv_filename=\"sample_submission.csv\"\n        path2csvLabels=os.path.join(data_dir,csv_filename)\n        labels_df=pd.read_csv(path2csvLabels)\n        \n        # set data frame index to id\n        labels_df.set_index(\"id\", inplace=True)\n        \n        # obtain labels from data frame\n        self.labels = [labels_df.loc[filename[:-4]].values[0] for filename in filenames]\n        self.transform = transform       \n        \n    def __len__(self):\n        # return size of dataset\n        return len(self.full_filenames)\n    \n    def __getitem__(self, idx):\n        # open image, apply transforms and return with label\n        image = Image.open(self.full_filenames[idx]) # PIL image\n        image = self.transform(image)\n        return image, self.labels[idx]","metadata":{"execution":{"iopub.status.busy":"2024-05-30T18:48:35.471556Z","iopub.execute_input":"2024-05-30T18:48:35.471966Z","iopub.status.idle":"2024-05-30T18:48:35.483097Z","shell.execute_reply.started":"2024-05-30T18:48:35.471936Z","shell.execute_reply":"2024-05-30T18:48:35.481991Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### **<span style='color:#F1A424'>CHECKS</span>**\n\n- Confirm our best performing model has been saved in the working directory \n- Confirm the test folder contents are present","metadata":{}},{"cell_type":"code","source":"!ls","metadata":{"execution":{"iopub.status.busy":"2024-05-30T18:48:38.726619Z","iopub.execute_input":"2024-05-30T18:48:38.727039Z","iopub.status.idle":"2024-05-30T18:48:39.866412Z","shell.execute_reply.started":"2024-05-30T18:48:38.727006Z","shell.execute_reply":"2024-05-30T18:48:39.864839Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!ls '/kaggle/input/histopathologic-cancer-detection/test' | head -n 5","metadata":{"execution":{"iopub.status.busy":"2024-05-30T18:48:43.645581Z","iopub.execute_input":"2024-05-30T18:48:43.646392Z","iopub.status.idle":"2024-05-30T18:48:50.265161Z","shell.execute_reply.started":"2024-05-30T18:48:43.646332Z","shell.execute_reply":"2024-05-30T18:48:50.263799Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### **<span style='color:#F1A424'>FUNCTION PARAMETER DICTIONARY</span>**\n\nHaving defined a model architecture, we can load model weights","metadata":{}},{"cell_type":"code","source":"# load any model weights for the model\ncnn_model.load_state_dict(torch.load('weights.pt'))","metadata":{"execution":{"iopub.status.busy":"2024-05-30T18:48:50.268672Z","iopub.execute_input":"2024-05-30T18:48:50.269657Z","iopub.status.idle":"2024-05-30T18:48:50.281798Z","shell.execute_reply.started":"2024-05-30T18:48:50.269610Z","shell.execute_reply":"2024-05-30T18:48:50.280712Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### **<span style='color:#F1A424'> TEST FILE IDS</span>**\n\nThe submission file contains all the ids to the files that are located in the test folder","metadata":{}},{"cell_type":"code","source":"# sample submission\npath_sub = \"/kaggle/input/histopathologic-cancer-detection/sample_submission.csv\"\nlabels_df = pd.read_csv(path_sub)\nlabels_df.head()\nlabels_df.shape","metadata":{"execution":{"iopub.status.busy":"2024-05-30T18:48:50.283004Z","iopub.execute_input":"2024-05-30T18:48:50.283351Z","iopub.status.idle":"2024-05-30T18:48:50.413600Z","shell.execute_reply.started":"2024-05-30T18:48:50.283323Z","shell.execute_reply":"2024-05-30T18:48:50.412456Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### **<span style='color:#F1A424'>TEST IMAGE DATASET</span>**\n\nLike we did with the training set, lets convert and store all image data in ","metadata":{}},{"cell_type":"code","source":"data_dir = '/kaggle/input/histopathologic-cancer-detection/'\n\ndata_transformer = transforms.Compose([transforms.ToTensor(),\n                                       transforms.Resize((46,46))])\n\nimg_dataset_test = pytorchdata_test(data_dir,data_transformer,data_type=\"test\")\nprint(len(img_dataset_test), 'samples found')","metadata":{"execution":{"iopub.status.busy":"2024-05-30T18:48:53.053844Z","iopub.execute_input":"2024-05-30T18:48:53.054251Z","iopub.status.idle":"2024-05-30T18:48:55.710203Z","shell.execute_reply.started":"2024-05-30T18:48:53.054224Z","shell.execute_reply":"2024-05-30T18:48:55.709001Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### **<span style='color:#F1A424'>PREDICTION FUNCTION</span>**\n\nFor inference, we need to set the model to `torch.no_grad`\n\n","metadata":{}},{"cell_type":"code","source":"def inference(model,dataset,device,num_classes=2):\n    \n    len_data=len(dataset)\n    y_out=torch.zeros(len_data,num_classes) # initialize output tensor on CPU\n    y_gt=np.zeros((len_data),dtype=\"uint8\") # initialize ground truth on CPU\n    model=model.to(device) # move model to device\n    \n    with torch.no_grad():\n        for i in tqdm(range(len_data)):\n            x,y=dataset[i]\n            y_gt[i]=y\n            y_out[i]=model(x.unsqueeze(0).to(device))\n\n    return y_out.numpy(),y_gt            ","metadata":{"execution":{"iopub.status.busy":"2024-05-30T18:48:55.712353Z","iopub.execute_input":"2024-05-30T18:48:55.712704Z","iopub.status.idle":"2024-05-30T18:48:55.720004Z","shell.execute_reply.started":"2024-05-30T18:48:55.712675Z","shell.execute_reply":"2024-05-30T18:48:55.719136Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_test_out,_ = inference(cnn_model,img_dataset_test, device)            ","metadata":{"execution":{"iopub.status.busy":"2024-05-30T18:48:55.805925Z","iopub.execute_input":"2024-05-30T18:48:55.808684Z","iopub.status.idle":"2024-05-30T18:49:51.825750Z","shell.execute_reply.started":"2024-05-30T18:48:55.808645Z","shell.execute_reply":"2024-05-30T18:49:51.824150Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# class predictions 0,1\ny_test_pred=np.argmax(y_test_out,axis=1)\nprint(y_test_pred.shape)\nprint(y_test_pred[0:5])","metadata":{"execution":{"iopub.status.busy":"2024-05-30T18:49:51.827248Z","iopub.status.idle":"2024-05-30T18:49:51.827926Z","shell.execute_reply.started":"2024-05-30T18:49:51.827723Z","shell.execute_reply":"2024-05-30T18:49:51.827743Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# probabilities of predicted selection\n# return F.log_softmax(x, dim=1) ie.\npreds = np.exp(y_test_out[:, 1])\nprint(preds.shape)\nprint(preds[0:5])","metadata":{"execution":{"iopub.status.busy":"2024-05-26T20:58:01.999868Z","iopub.status.idle":"2024-05-26T20:58:02.000209Z","shell.execute_reply.started":"2024-05-26T20:58:02.000041Z","shell.execute_reply":"2024-05-26T20:58:02.000060Z"},"trusted":true},"execution_count":null,"outputs":[]}]}